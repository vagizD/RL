{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aA14kYCYSwo"
   },
   "source": [
    "## Динамическое программирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roQ4hAstjVce"
   },
   "source": [
    "Рассмотрим алгоритм итерации по оценкам состояния $V$ (Value Iteration):\n",
    "$$\n",
    "V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]\n",
    "$$\n",
    "На основе оценки $V_i$ можно посчитать функцию оценки $Q_i$ действия $a$ в состоянии $s$:\n",
    "$$\n",
    "Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]\n",
    "$$\n",
    "$$\n",
    "V_{(i+1)}(s) = \\max_a Q_i(s,a)\n",
    "$$\n",
    "\n",
    "Зададим напрямую модель MDP с картинки:\n",
    "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.png\" caption=\"Марковский процесс принятия решений\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l6LwgNvgYXIP"
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OpKyGJEJYYDn"
   },
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "  's0':{\n",
    "    'a0': {'s0': 0.5, 's2': 0.5},\n",
    "    'a1': {'s2': 1}\n",
    "  },\n",
    "  's1':{\n",
    "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "    'a1': {'s1': 0.95, 's2': 0.05}\n",
    "  },\n",
    "  's2':{\n",
    "    'a0': {'s0': 0.4, 's2': 0.6},\n",
    "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "  }\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "import numpy as np\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVSC6KXuYcsh"
   },
   "source": [
    "Теперь мы можем использовать это MDP, как и любое другое gym окружение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PzLyFJ4iYfro"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state = s0\n",
      "next_state = s2, reward = 0.0, done = False\n"
     ]
    }
   ],
   "source": [
    "state = mdp.reset()\n",
    "print('initial state =', state)\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print(f'next_state = {next_state}, reward = {reward}, done = {done}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgRdVPJlYjZ4"
   },
   "source": [
    ":Также, помимо стандартных методов, есть дополнительные, которые пригодятся нам для реализации метода итерации по полезностям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4zK1xXedYn21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_states = ('s0', 's1', 's2')\n",
      "possible_actions('s1') =  ('a0', 'a1')\n",
      "next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
      "reward('s1', 'a0', 's0') =  5\n",
      "transition_prob('s1', 'a0', 's0') =  0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"all_states =\", mdp.get_all_states())\n",
    "print(\"possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"reward('s1', 'a0', 's0') = \",mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"transition_prob('s1', 'a0', 's0') = \",\n",
    "      mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Oe_RzZtYq11"
   },
   "source": [
    "### Задание 1\n",
    "\n",
    "Теперь реализуем алгоритм итерации по полезностям, чтобы решить этот вручную заданный MDP. Псевдокод алгоритма:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Инициализируем $V^{(0)}(s)=0$, для всех $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    "\n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, для всех $s$\n",
    "\n",
    "---\n",
    "\n",
    "Вначале вычисляем оценку состояния-действия:\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aA0DQccjody"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Qt0o0MokYv0F"
   },
   "outputs": [],
   "source": [
    "def get_action_value(\n",
    "    mdp, state_values, state, action, gamma\n",
    "):\n",
    "    \"\"\" Вычисляем Q(s,a) по формуле выше \"\"\"\n",
    "    Q = 0\n",
    "    for next_state, next_state_value in state_values.items():\n",
    "        prob   = mdp.get_transition_prob(state, action, next_state)\n",
    "        reward = mdp.get_reward(state, action, next_state)\n",
    "        value  = gamma * next_state_value\n",
    "        Q += prob * (reward + value)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "x06WscSIYysp"
   },
   "outputs": [],
   "source": [
    "test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "assert np.isclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
    "assert np.isclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87q6GhsMY19h"
   },
   "source": [
    "Теперь оцениваем полезность самого состояния, для этого мы можем использовать предыдущий метод:\n",
    "\n",
    "$$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O3QFuoVj1iZ"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hFqCuRaBY5J_"
   },
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Считаем следующее V(s) по формуле выше.\"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return 0\n",
    "    V = max(\n",
    "        get_action_value(mdp, state_values, state, action, gamma)\n",
    "        for action in mdp.get_possible_actions(state)\n",
    "    )\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lPUyRzQOY8PP"
   },
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.isclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
    "assert np.isclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 1.08)\n",
    "assert np.isclose(get_new_state_value(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9), -13500000000.0), \\\n",
    "   \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n",
    "assert test_Vs == test_Vs_copy, \"Убедитесь, что вы не изменяете state_values в функции get_new_state_value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od-SBiPKY_Q3"
   },
   "source": [
    "Теперь создаем основной цикл итерационного оценки полезности состояний с критерием остановки, который проверяет насколько изменились полезности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-0-PlvmkF7P"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uUwb5JCDZDD4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0 | diff: 3.50000 | V(start): 0.000 \n",
      "iter    1 | diff: 0.64500 | V(start): 0.000 \n",
      "iter    2 | diff: 0.58050 | V(start): 0.581 \n",
      "iter    3 | diff: 0.43582 | V(start): 0.866 \n",
      "iter    4 | diff: 0.30634 | V(start): 1.145 \n",
      "iter    5 | diff: 0.27571 | V(start): 1.421 \n",
      "iter    6 | diff: 0.24347 | V(start): 1.655 \n",
      "iter    7 | diff: 0.21419 | V(start): 1.868 \n",
      "iter    8 | diff: 0.19277 | V(start): 2.061 \n",
      "iter    9 | diff: 0.17327 | V(start): 2.233 \n",
      "iter   10 | diff: 0.15569 | V(start): 2.389 \n",
      "iter   11 | diff: 0.14012 | V(start): 2.529 \n",
      "iter   12 | diff: 0.12610 | V(start): 2.655 \n",
      "iter   13 | diff: 0.11348 | V(start): 2.769 \n",
      "iter   14 | diff: 0.10213 | V(start): 2.871 \n",
      "iter   15 | diff: 0.09192 | V(start): 2.963 \n",
      "iter   16 | diff: 0.08272 | V(start): 3.045 \n",
      "iter   17 | diff: 0.07445 | V(start): 3.120 \n",
      "iter   18 | diff: 0.06701 | V(start): 3.187 \n",
      "iter   19 | diff: 0.06031 | V(start): 3.247 \n",
      "iter   20 | diff: 0.05428 | V(start): 3.301 \n",
      "iter   21 | diff: 0.04885 | V(start): 3.350 \n",
      "iter   22 | diff: 0.04396 | V(start): 3.394 \n",
      "iter   23 | diff: 0.03957 | V(start): 3.434 \n",
      "iter   24 | diff: 0.03561 | V(start): 3.469 \n",
      "iter   25 | diff: 0.03205 | V(start): 3.502 \n",
      "iter   26 | diff: 0.02884 | V(start): 3.530 \n",
      "iter   27 | diff: 0.02596 | V(start): 3.556 \n",
      "iter   28 | diff: 0.02336 | V(start): 3.580 \n",
      "iter   29 | diff: 0.02103 | V(start): 3.601 \n",
      "iter   30 | diff: 0.01892 | V(start): 3.620 \n",
      "iter   31 | diff: 0.01703 | V(start): 3.637 \n",
      "iter   32 | diff: 0.01533 | V(start): 3.652 \n",
      "iter   33 | diff: 0.01380 | V(start): 3.666 \n",
      "iter   34 | diff: 0.01242 | V(start): 3.678 \n",
      "iter   35 | diff: 0.01117 | V(start): 3.689 \n",
      "iter   36 | diff: 0.01006 | V(start): 3.699 \n",
      "iter   37 | diff: 0.00905 | V(start): 3.708 \n",
      "iter   38 | diff: 0.00815 | V(start): 3.717 \n",
      "iter   39 | diff: 0.00733 | V(start): 3.724 \n",
      "iter   40 | diff: 0.00660 | V(start): 3.731 \n",
      "iter   41 | diff: 0.00594 | V(start): 3.736 \n",
      "iter   42 | diff: 0.00534 | V(start): 3.742 \n",
      "iter   43 | diff: 0.00481 | V(start): 3.747 \n",
      "iter   44 | diff: 0.00433 | V(start): 3.751 \n",
      "iter   45 | diff: 0.00390 | V(start): 3.755 \n",
      "iter   46 | diff: 0.00351 | V(start): 3.758 \n",
      "iter   47 | diff: 0.00316 | V(start): 3.762 \n",
      "iter   48 | diff: 0.00284 | V(start): 3.764 \n",
      "iter   49 | diff: 0.00256 | V(start): 3.767 \n",
      "iter   50 | diff: 0.00230 | V(start): 3.769 \n",
      "iter   51 | diff: 0.00207 | V(start): 3.771 \n",
      "iter   52 | diff: 0.00186 | V(start): 3.773 \n",
      "iter   53 | diff: 0.00168 | V(start): 3.775 \n",
      "iter   54 | diff: 0.00151 | V(start): 3.776 \n",
      "iter   55 | diff: 0.00136 | V(start): 3.778 \n",
      "iter   56 | diff: 0.00122 | V(start): 3.779 \n",
      "iter   57 | diff: 0.00110 | V(start): 3.780 \n",
      "iter   58 | diff: 0.00099 | V(start): 3.781 \n",
      "Принято! Алгоритм сходится!\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(\n",
    "    mdp, state_values=None,\n",
    "    gamma = 0.9, num_iter = 1000, min_difference = 1e-5\n",
    "):\n",
    "    \"\"\" выполняет num_iter шагов итерации по значениям\"\"\"\n",
    "    # инициализируем V(s)\n",
    "    state_values = state_values or \\\n",
    "    {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        # Вычисляем новые полезности состояний,\n",
    "        # используя функции, определенные выше.\n",
    "        new_state_values = {\n",
    "            state: get_new_state_value(mdp, state_values, state, gamma)\n",
    "            for state in mdp.get_all_states()\n",
    "        }\n",
    "\n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Считаем разницу\n",
    "        diff = max(\n",
    "            abs(new_state_values[s] - state_values[s])\n",
    "            for s in mdp.get_all_states()\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"iter {i:4} | diff: {diff:6.5f} \"\n",
    "            f\"| V(start): {new_state_values[mdp._initial_state]:.3f} \"\n",
    "        )\n",
    "\n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            print(\"Принято! Алгоритм сходится!\")\n",
    "            break\n",
    "\n",
    "    return state_values\n",
    "\n",
    "state_values = value_iteration(\n",
    "    mdp, num_iter = 100, min_difference = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZKomkPSrZGlZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final state values: {'s0': 3.7810348735476405, 's1': 7.294006423867229, 's2': 4.202140275227048}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final state values:\", state_values)\n",
    "\n",
    "assert abs(state_values['s0'] - 3.781) < 0.01\n",
    "assert abs(state_values['s1'] - 7.294) < 0.01\n",
    "assert abs(state_values['s2'] - 4.202) < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gz5JxncZJoX"
   },
   "source": [
    "По найденным полезностям и зная модель переходов легко найти оптимальную стратегию:\n",
    "$$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ml9AWeYrkNgf"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7gd4m26TZOn3"
   },
   "outputs": [],
   "source": [
    "def get_optimal_action(\n",
    "    mdp, state_values, state, gamma=0.9\n",
    "):\n",
    "    \"\"\" Находим оптимальное действие, используя формулу выше. \"\"\"\n",
    "    if mdp.is_terminal(state): return None\n",
    "\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    i = np.argmax([\n",
    "        get_action_value(mdp, state_values, state, actions[i], gamma)\n",
    "        for i in range(len(actions))\n",
    "    ])\n",
    "    return actions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yv3MRqaQZSzs"
   },
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, state_values, 's0', 0.9) == 'a1'\n",
    "assert get_optimal_action(mdp, state_values, 's1', 0.9) == 'a0'\n",
    "assert get_optimal_action(mdp, state_values, 's2', 0.9) == 'a1'\n",
    "\n",
    "assert get_optimal_action(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9) == 'a0', \\\n",
    "    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n",
    "assert get_optimal_action(mdp, {'s0': -2e10, 's1': 0, 's2': -1e10}, 's0', 0.9) == 'a1', \\\n",
    "    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "V1KMZyhbZVVX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward:  0.4704\n"
     ]
    }
   ],
   "source": [
    "# Проверим среднее вознаграждение агента\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, 0.9))\n",
    "    rewards.append(r)\n",
    "\n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert(0.40 < np.mean(rewards) < 0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkokwmulZYYn"
   },
   "source": [
    "### Задание 2\n",
    "\n",
    "Теперь проверим работу итерации по ценностям на классической задаче FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "E4V34IMzZbeH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*FFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "iter    0 | diff: 1.00000 | V(start): 0.000 \n",
      "iter    1 | diff: 0.90000 | V(start): 0.000 \n",
      "iter    2 | diff: 0.81000 | V(start): 0.000 \n",
      "iter    3 | diff: 0.72900 | V(start): 0.000 \n",
      "iter    4 | diff: 0.65610 | V(start): 0.000 \n",
      "iter    5 | diff: 0.59049 | V(start): 0.590 \n",
      "iter    6 | diff: 0.00000 | V(start): 0.590 \n",
      "Принято! Алгоритм сходится!\n"
     ]
    }
   ],
   "source": [
    "from mdp import FrozenLakeEnv\n",
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "\n",
    "mdp.render()\n",
    "state_values = value_iteration(mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNPtPQo2ZdpV"
   },
   "source": [
    "Визуализируем нашу стратегию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aQP4HnjNZg4C"
   },
   "outputs": [],
   "source": [
    "def draw_policy(mdp, state_values, gamma=0.9):\n",
    "    \"\"\"функция визуализации стратегии\"\"\"\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    h, w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {\n",
    "        s: get_optimal_action(mdp, state_values, s, gamma)\n",
    "        for s in states\n",
    "    }\n",
    "    plt.imshow(\n",
    "        V.reshape(w, h),\n",
    "        cmap='gray', interpolation='none',\n",
    "        clim=(0, 1)\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h) - .5)\n",
    "    ax.set_yticks(np.arange(w) - .5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down': (0, -1),\n",
    "            'right': (1, 0), 'up': (-1, 0)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
    "                     color='g', size=12,\n",
    "                     verticalalignment='center',\n",
    "                     horizontalalignment='center',\n",
    "                     fontweight='bold')\n",
    "            a = Pi[y, x]\n",
    "            if a is None: continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y, u * .3, -v * .3,\n",
    "                      color='m', head_width=0.1,\n",
    "                      head_length=0.1)\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UJ2zkkx2Zlec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 29\n",
      "iter    0 | diff: 0.00000 | V(start): 0.198 \n",
      "Принято! Алгоритм сходится!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mdp = FrozenLakeEnv(map_name='8x8',slip_chance=0.1)\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(30):\n",
    "    clear_output(True)\n",
    "    print(\"after iteration %i\"%i)\n",
    "    state_values = value_iteration(mdp,\n",
    "                            state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "    sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nltqOBDfZoFG"
   },
   "source": [
    "Посмотрим на оптимальную стратегию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CKJ1oJapZq77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*FFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "S*FFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SF*FFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFF*FFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFF*FFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFFF*FF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFFFF*F\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFF*FF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFFFF*F\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFF*F\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFF*F\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFH*F\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF*\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF*\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH*\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH*\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = mdp.reset()\n",
    "mdp.render()\n",
    "for t in range(100):\n",
    "    a = get_optimal_action(mdp, state_values, s, 0.9)\n",
    "    print(a, end='\\n\\n')\n",
    "    s, r, done, _ = mdp.step(a)\n",
    "    mdp.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksq-NonlZtHM"
   },
   "source": [
    "Тестируем на более сложном варианте окружения:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6g4fbKkkZza"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yOBqWNBfZv6v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0 | diff: 0.80000 | V(start): 0.000 \n",
      "iter    1 | diff: 0.57600 | V(start): 0.000 \n",
      "iter    2 | diff: 0.41472 | V(start): 0.000 \n",
      "iter    3 | diff: 0.29860 | V(start): 0.000 \n",
      "iter    4 | diff: 0.24186 | V(start): 0.000 \n",
      "iter    5 | diff: 0.19349 | V(start): 0.000 \n",
      "iter    6 | diff: 0.15325 | V(start): 0.000 \n",
      "iter    7 | diff: 0.12288 | V(start): 0.000 \n",
      "iter    8 | diff: 0.09930 | V(start): 0.000 \n",
      "iter    9 | diff: 0.08037 | V(start): 0.000 \n",
      "iter   10 | diff: 0.06426 | V(start): 0.000 \n",
      "iter   11 | diff: 0.05129 | V(start): 0.000 \n",
      "iter   12 | diff: 0.04330 | V(start): 0.000 \n",
      "iter   13 | diff: 0.03802 | V(start): 0.033 \n",
      "iter   14 | diff: 0.03332 | V(start): 0.058 \n",
      "iter   15 | diff: 0.02910 | V(start): 0.087 \n",
      "iter   16 | diff: 0.01855 | V(start): 0.106 \n",
      "iter   17 | diff: 0.01403 | V(start): 0.120 \n",
      "iter   18 | diff: 0.00810 | V(start): 0.128 \n",
      "iter   19 | diff: 0.00555 | V(start): 0.133 \n",
      "iter   20 | diff: 0.00321 | V(start): 0.137 \n",
      "iter   21 | diff: 0.00247 | V(start): 0.138 \n",
      "iter   22 | diff: 0.00147 | V(start): 0.139 \n",
      "iter   23 | diff: 0.00104 | V(start): 0.140 \n",
      "iter   24 | diff: 0.00058 | V(start): 0.140 \n",
      "iter   25 | diff: 0.00036 | V(start): 0.141 \n",
      "iter   26 | diff: 0.00024 | V(start): 0.141 \n",
      "iter   27 | diff: 0.00018 | V(start): 0.141 \n",
      "iter   28 | diff: 0.00012 | V(start): 0.141 \n",
      "iter   29 | diff: 0.00007 | V(start): 0.141 \n",
      "iter   30 | diff: 0.00004 | V(start): 0.141 \n",
      "iter   31 | diff: 0.00003 | V(start): 0.141 \n",
      "iter   32 | diff: 0.00001 | V(start): 0.141 \n",
      "iter   33 | diff: 0.00001 | V(start): 0.141 \n",
      "Принято! Алгоритм сходится!\n",
      "Cреднее вознаграждение: 0.717\n",
      "Принято!\n"
     ]
    }
   ],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    state = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        action = get_optimal_action(mdp, state_values, state, 0.8)\n",
    "        next_state, reward, done, _ = mdp.step(action)\n",
    "        state = next_state\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"Cреднее вознаграждение:\", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
    "print(\"Принято!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOEo-OheZyYP"
   },
   "source": [
    "### Задание 3\n",
    "\n",
    "Теперь рассмотрим алгоритм итерации по стратегиям (PI, policy iteration):\n",
    "\n",
    "---\n",
    "Initialize $\\pi_0$   `// случайно`\n",
    "\n",
    "For $n=0, 1, 2, \\dots$\n",
    "- Считаем функцию $V^{\\pi_{n}}$\n",
    "- Используя $V^{\\pi_{n}}$, считаем функцию $Q^{\\pi_{n}}$\n",
    "- Получаем новую стратегию: $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
    "---\n",
    "\n",
    "PI включает в себя оценку полезности состояния, как внутренний шаг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Diaeh1f7Z010"
   },
   "source": [
    "Вначале оценим полезности, используя текущую стратегию:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "    Мы будем искать точное решение, хотя могли использовать и предыдущий итерационный подход. Для этого будем решать систему линейных уравнений относительно $V^{\\pi}(s_i)$ с помощью np.linalg.solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pFDjkE2kfsY"
   },
   "source": [
    "### 3 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-RpV4Yw8Z3bi"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import solve\n",
    "\n",
    "def compute_vpi(mdp, policy, gamma):\n",
    "    \"\"\"\n",
    "    Считем V^pi(s) для всех состояний, согласно стратегии.\n",
    "    :param policy: словарь состояние->действие {s : a}\n",
    "    :returns: словарь {state : V^pi(state)}\n",
    "    \"\"\"\n",
    "    states = mdp.get_all_states()\n",
    "    n = len(states)\n",
    "    A, b = [], []\n",
    "    for i, state in enumerate(states):\n",
    "        # V(s)\n",
    "        ohe = np.zeros(n)\n",
    "        ohe[i] = 1\n",
    "        if state in policy:\n",
    "            action = policy[state]\n",
    "            \n",
    "            # probabilities\n",
    "            probs = np.array([\n",
    "                mdp.get_transition_prob(state, action, next_state)\n",
    "                for next_state in states\n",
    "            ])\n",
    "\n",
    "            # rewards\n",
    "            rewards = np.array([\n",
    "                mdp.get_reward(state, action, next_state)\n",
    "                for next_state in states\n",
    "            ])\n",
    "        else:\n",
    "            probs = np.zeros(n)\n",
    "            rewards = np.zeros(n)\n",
    "        row = ohe - gamma * probs\n",
    "        value = np.dot(probs, rewards)\n",
    "        A.append(row)\n",
    "        b.append(value)\n",
    "\n",
    "    A = np.array(A)\n",
    "    b = np.array(b)\n",
    "    values = solve(A, b)\n",
    "\n",
    "    state_values = {\n",
    "        states[i] : values[i]\n",
    "        for i in range(len(states))\n",
    "    }\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "qeb79E20Z6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': np.float64(-0.8620499834856328), 's1': np.float64(-0.2972586149950459), 's2': np.float64(-0.9578333149840365)}\n"
     ]
    }
   ],
   "source": [
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's1': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "test_policy = {\n",
    "    s: np.random.choice(mdp.get_possible_actions(s))\n",
    "    for s in mdp.get_all_states()}\n",
    "new_vpi = compute_vpi(mdp, test_policy, gamma)\n",
    "\n",
    "print(new_vpi)\n",
    "assert type(new_vpi) is dict, \\\n",
    "    \"функция compute_vpi должна возвращать словарь \\\n",
    "    {состояние s : V^pi(s) }\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du2YNXpxZ9BT"
   },
   "source": [
    "Теперь обновляем стратегию на основе новых значений полезностей:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsYlHPblkrSI"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TGCHMeaUZ_Qj"
   },
   "outputs": [],
   "source": [
    "def compute_new_policy(mdp, vpi, gamma):\n",
    "    \"\"\"\n",
    "    Рассчитываем новую стратегию\n",
    "    :param vpi: словарь {state : V^pi(state) }\n",
    "    :returns: словарь {state : оптимальное действие}\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "    for state in mdp.get_all_states():\n",
    "        Q[state] = {}\n",
    "        for a in mdp.get_possible_actions(state):\n",
    "            values = []\n",
    "            for next_state in mdp.get_next_states(state, a):\n",
    "                r = mdp.get_reward(state, a, next_state)\n",
    "                p = mdp.get_transition_prob(\n",
    "                    state, a, next_state\n",
    "                )\n",
    "                values.append(p * (r + vpi[next_state]))\n",
    "\n",
    "            Q[state][a] = sum(values)\n",
    "\n",
    "    policy = {}\n",
    "    for state in mdp.get_all_states():\n",
    "        actions = mdp.get_possible_actions(state)\n",
    "        if actions:\n",
    "           policy[state] = get_optimal_action(mdp, vpi, state, gamma)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1b1OXlg9aBsy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': 'a0', 's1': 'a0', 's2': 'a0'}\n"
     ]
    }
   ],
   "source": [
    "new_policy = compute_new_policy(mdp, new_vpi, gamma)\n",
    "\n",
    "print(new_policy)\n",
    "\n",
    "assert type(new_policy) is dict, \\\n",
    "\"функция compute_new_policy должна возвращать словарь \\\n",
    "{состояние s: оптимальное действие}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15mJglOZaEmI"
   },
   "source": [
    "Собираем все в единый цикл:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIrgcIqKkxD2"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "2LcLHHhIaHAZ"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(\n",
    "    mdp, policy=None, gamma = 0.9,\n",
    "    num_iter = 1000, min_difference = 1e-5\n",
    "):\n",
    "    \"\"\"\n",
    "    Запускаем цикл итерации по стратегиям\n",
    "    Если стратегия не определена, задаем случайную\n",
    "    \"\"\"\n",
    "    for i in range(num_iter):\n",
    "        if not policy:\n",
    "            policy = {}\n",
    "            for s in mdp.get_all_states():\n",
    "                if mdp.get_possible_actions(s):\n",
    "                    policy[s] = (\n",
    "                        np.random.choice(mdp.get_possible_actions(s))\n",
    "                    )\n",
    "        vpi = compute_vpi(mdp, policy, gamma)\n",
    "        policy = compute_new_policy(mdp, vpi, gamma)\n",
    "    return vpi, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddfLTSfgaJjU"
   },
   "source": [
    "Тестируем на FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4hLv3X0OaKmg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward:  0.875\n",
      "Принято!\n"
     ]
    }
   ],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
    "state_values, policy = policy_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(policy[s])\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
    "print(\"Принято!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
