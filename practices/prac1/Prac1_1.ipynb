{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RL basics\n",
    "\n",
    "Термины и понятия:\n",
    "\n",
    "- агент/среда\n",
    "- наблюдение $o$ / состояние $s$\n",
    "- действие $a$, стратегия $\\pi: \\pi(s) \\rightarrow a$ функция перехода $T: T(s, a) \\rightarrow s'$\n",
    "- вознаграждение $r$, ф-я вознаграждений $R: R(s, a) \\rightarrow r$\n",
    "- цикл взаимодействия, траектория $\\tau: (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T)$, эпизод\n",
    "- отдача $G$, подсчет отдачи, средняя[/ожидаемая] отдача $\\mathbb{E}[G]$"
   ],
   "metadata": {
    "id": "k4NMHBq16Y_g"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    COLAB = False\n",
    "    pass\n",
    "\n",
    "if COLAB:\n",
    "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
    "    !pip -q install piglet\n",
    "    !pip -q install imageio_ffmpeg\n",
    "    !pip -q install moviepy==1.0.3"
   ],
   "metadata": {
    "id": "MoNP7Wdn6aP0",
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:22.547459Z",
     "start_time": "2025-09-27T10:10:22.544526Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "id": "9JPaLF5v6esZ",
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:22.855887Z",
     "start_time": "2025-09-27T10:10:22.587686Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent, environment\n",
    "\n",
    "<img src=https://gymnasium.farama.org/_images/lunar_lander.gif caption=\"lunar lander\" width=\"150\" height=\"50\"><img src=https://gymnasium.farama.org/_images/mountain_car.gif caption=\"mountain car\" width=\"150\" height=\"50\">\n",
    "<img src=https://gymnasium.farama.org/_images/cliff_walking.gif caption=\"cliff walking\" width=\"300\" height=\"50\">\n",
    "<img src=https://ale.farama.org/_images/montezuma_revenge.gif caption=\"montezuma revenge\" width=\"150\" height=\"100\">\n",
    "<img src=https://github.com/danijar/crafter/raw/main/media/video.gif caption=\"crafter\" width=\"150\" height=\"100\">\n",
    "<img src=https://camo.githubusercontent.com/6df2ca438d8fe8aa7a132b859315147818c54af608f8609320c3c20e938acf48/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f344e78376759694d394e44724d724d616f372f67697068792e676966 caption=\"malmo minecraft\" width=\"150\" height=\"100\">\n",
    "<img src=https://images.ctfassets.net/kftzwdyauwt9/e0c0947f-1a44-4528-4a41450a9f0a/2d0e85871d58d02dbe01b2469d693d4a/table-03.gif caption=\"roboschool\" width=\"150\" height=\"100\">\n",
    "<img src=https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.png caption=\"Марковский процесс принятия решений\" width=\"150\" height=\"100\">\n",
    "<img src=https://minigrid.farama.org/_images/DoorKeyEnv.gif caption=\"minigrid\" width=\"120\" height=\"120\">"
   ],
   "metadata": {
    "id": "OGAoJeNF6hJO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Observation, state\n",
    "\n",
    "TODO:\n",
    "- добавить примеры наблюдений/состояний (числа, векторы, картинки)\n",
    "- интуитивное объяснение различия, положить пока, что наблюдение = состояние\n",
    "- пространство состояний\n",
    "\n",
    "\n",
    "В каждый момент времени среда имеет некоторое внутреннее состояние. Здесь слово \"состояние\" я употребил скорее в интуитивном понимании, чтобы обозначить, что среда изменчива (иначе какой смысл с ней взаимодействовать, если ничего не меняется). В обучении с подкреплением под термином состояние $s$ (или $s_t$, где $t$ — текущее время) подразумевают либо абстрактно информацию о \"состоянии\" среды, либо ее явное представление в виде данных, достаточные для полного описания \"состояния\". *NB: Здесь можно провести аналогию с компьютерными играми — файл сохранения игры как раз содержит информацию о \"состоянии\" мира игры, чтобы в будущем можно было продолжить с текущей точки, так что данные этого файла в целом можно с некоторой натяжкой считать состоянием (с натяжкой, потому что редко когда в сложных играх файлы сохранения содержат прямо вот всю информацию, так что после перезагрузки вы получите не совсем точную копию). При этом обычно подразумевается, что состояние не содержит в себе ничего лишнего, то есть это **минимальный** набор информации.*\n",
    "\n",
    "Наблюдением $o$ называют то, что агент \"видит\" о текущем состоянии среды. Это не обязательно зрение, а вообще вся доступная ему информация (условно, со всех его органов чувств).\n",
    "\n",
    "В общем случае наблюдение: кортеж/словарь многомерных векторов чисел."
   ],
   "metadata": {
    "id": "dcyLKga76mA_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(gym.make(\"CartPole-v0\").reset()[0].shape)\n",
    "print(gym.make(\"MountainCar-v0\").reset()[0].shape)"
   ],
   "metadata": {
    "id": "ypHv9w6i6pcX",
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:22.907545Z",
     "start_time": "2025-09-27T10:10:22.903674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vagiz/Desktop/desktop_vagiz/HSE/Programming/RL/.venv/lib/python3.12/site-packages/gymnasium/envs/registration.py:512: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Action, policy, transition function\n",
    "\n",
    "Рассмотрим следующие MDP:\n",
    "\n",
    "- A: <img src=https://i.ibb.co/mrCMVZLQ/mdp-a.png caption=\"A\" width=\"400\" height=\"100\">\n",
    "- B: <img src=https://i.ibb.co/GQ2tVtjC/mdp-b.png caption=\"B\" width=\"400\" height=\"100\">\n",
    "\n",
    "Links to all:\n",
    "[A](https://i.ibb.co/mrCMVZLQ/mdp-a.png)\n",
    "[B](https://i.ibb.co/GQ2tVtjC/mdp-b.png)\n",
    "[C](https://i.ibb.co/Jj9LYHjP/mdp-c.png)\n",
    "[D](https://i.ibb.co/Y47Mr83b/mdp-d.png)\n",
    "[E](https://i.ibb.co/Kjt1Xhmf/mdp-e.png)\n",
    "\n",
    "Давайте явно запишем пространства состояний $S$ и действий $A$, а также функцию перехода $T$ среды.\n",
    "\n",
    "$$T(s_t, a_t) \\mapsto s_{t+1}$$"
   ],
   "metadata": {
    "id": "GabuCLcJ67lb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "states = set(range(3))\n",
    "actions = set(range(1))\n",
    "\n",
    "print(f'{states=} | {actions=}')\n",
    "\n",
    "\n",
    "T = {  # (s, a)\n",
    "    (0, 0): 1,\n",
    "    (1, 0): 2,\n",
    "    (2, 0): 2\n",
    "}\n",
    "print(f'Transition function {T=}')\n",
    "\n",
    "A_mdp = states, actions, T"
   ],
   "metadata": {
    "id": "4XpqNc_o6_CS",
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:22.922799Z",
     "start_time": "2025-09-27T10:10:22.918803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states={0, 1, 2} | actions={0}\n",
      "Transition function T={(0, 0): 1, (1, 0): 2, (2, 0): 2}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуйте записать функцию перехода в матричном виде:"
   ],
   "metadata": {
    "id": "KlqB4WcZ7CDK"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:22.974640Z",
     "start_time": "2025-09-27T10:10:22.969446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_transition_prob(T, a, s1, s2):\n",
    "    num = sum(1 for k, v in T.items() if k == (s1, a) and v == s2)\n",
    "    den = sum(1 for k, v in T.items() if k == (s1, a))\n",
    "    return num / den if den > 0 else 0\n",
    "\n",
    "def get_transition_matrix(A_mdp):\n",
    "    states, actions, T = A_mdp\n",
    "    n, m = len(actions), len(states)\n",
    "    \n",
    "    P = np.zeros((n, m, m))\n",
    "    \n",
    "    for a in range(n):\n",
    "        for s1 in states:\n",
    "            for s2 in states:\n",
    "                P[a, s1, s2] = get_transition_prob(T, a, s1, s2)\n",
    "    return P\n",
    "    \n",
    "P = get_transition_matrix(A_mdp)\n",
    "P"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как получить вероятность нахождения агента в состоянии (1) через N шагов? Что происходит с вероятностями нахождения в состояниях при $N \\rightarrow \\infty$\n",
    "\n",
    "Вероятности считаются рекурсивно:\n",
    "\n",
    "$$\\mathbb{P}(s_N = s') = \\sum_{s \\in S}\\sum_{a \\in A}\\pi(a|s)\\mathbb{P}(s_N = s' | s_{N - 1} = s, a_{N - 1} = a)\\mathbb{P}(s_{N-1} = s)$$\n",
    "\n",
    "* первый множитель: $\\pi$, не зависит от времени\n",
    "* второй множитель: чиселко $P_{a,s,s'}$ из матрицы переходов\n",
    "* третий множитель: вероятности на предыдущем шаге оказаться в $s$, рекурсия в этом месте\n",
    "\n",
    "Будем считать, что агент ходит равновероятно по любому ребру. Для подсчетов будет достаточно. Реализация неэффективная, так как можно было бы через ДП посчитать или кэшировать функцию."
   ],
   "metadata": {
    "id": "_FPshg_07G0R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_prob_of_state(A_mdp, P, first_state, k, N):  # вероятность быть в состоянии (k) после N шагов\n",
    "    if N == first_state:\n",
    "        return float(k == first_state)\n",
    "\n",
    "    prob = 0\n",
    "    states, actions, T = A_mdp\n",
    "    for s in states:\n",
    "        n_actions = sum(1 for key in T if key[0] == s)  # число возможных действий из состояния s\n",
    "        if n_actions == 0:\n",
    "            continue\n",
    "\n",
    "        cur_prob = 0\n",
    "        for a in actions:\n",
    "            cur_prob += P[a, s, k] * get_prob_of_state(A_mdp, P, first_state, s, N - 1)  # вероятность быть в s на шаге N - 1\n",
    "        prob += 1 / n_actions * cur_prob\n",
    "    return prob"
   ],
   "metadata": {
    "id": "9pwc4Atn7IAf",
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:23.025095Z",
     "start_time": "2025-09-27T10:10:23.022464Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:23.071715Z",
     "start_time": "2025-09-27T10:10:23.069266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k = 1\n",
    "for N in range(5):\n",
    "    print(f\"Вероятность оказаться в состоянии ({k}) через {N} шагов: {get_prob_of_state(A_mdp, P, 0, k, N):.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вероятность оказаться в состоянии (1) через 0 шагов: 0.00\n",
      "Вероятность оказаться в состоянии (1) через 1 шагов: 1.00\n",
      "Вероятность оказаться в состоянии (1) через 2 шагов: 0.00\n",
      "Вероятность оказаться в состоянии (1) через 3 шагов: 0.00\n",
      "Вероятность оказаться в состоянии (1) через 4 шагов: 0.00\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Результат оправданный - мы гарантированно будем в $(1)$ после первого шага, а затем выйдем и не вернемся в него."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- B: <img src=https://i.ibb.co/GQ2tVtjC/mdp-b.png caption=\"B\" width=\"400\" height=\"100\">\n",
    "\n",
    "Аналогичное для $B$:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:23.120860Z",
     "start_time": "2025-09-27T10:10:23.118116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "states = set(range(4))\n",
    "actions = set(range(3))\n",
    "\n",
    "print(f'{states=} | {actions=}')\n",
    "\n",
    "\n",
    "T = {  # (s, a)\n",
    "    (0, 0): 1,\n",
    "    (0, 1): 2,\n",
    "    (0, 2): 3,\n",
    "    \n",
    "    (1, 0): 1,\n",
    "    (1, 1): 1,\n",
    "    (1, 2): 1,\n",
    "\n",
    "    (2, 0): 2,\n",
    "    (2, 1): 2,\n",
    "    (2, 2): 2,\n",
    "\n",
    "    (3, 0): 3,\n",
    "    (3, 1): 3,\n",
    "    (3, 2): 3,\n",
    "}\n",
    "print(f'Transition function {T=}')\n",
    "\n",
    "A_mdp = states, actions, T"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states={0, 1, 2, 3} | actions={0, 1, 2}\n",
      "Transition function T={(0, 0): 1, (0, 1): 2, (0, 2): 3, (1, 0): 1, (1, 1): 1, (1, 2): 1, (2, 0): 2, (2, 1): 2, (2, 2): 2, (3, 0): 3, (3, 1): 3, (3, 2): 3}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:23.170410Z",
     "start_time": "2025-09-27T10:10:23.167744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "P = get_transition_matrix(A_mdp)\n",
    "P"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:23.221216Z",
     "start_time": "2025-09-27T10:10:23.219014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for N in range(2):\n",
    "    for k in range(4):\n",
    "        print(f\"Вероятность оказаться в состоянии ({k}) через {N} шагов: {get_prob_of_state(A_mdp, P, 0, k, N):.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вероятность оказаться в состоянии (0) через 0 шагов: 1.00\n",
      "Вероятность оказаться в состоянии (1) через 0 шагов: 0.00\n",
      "Вероятность оказаться в состоянии (2) через 0 шагов: 0.00\n",
      "Вероятность оказаться в состоянии (3) через 0 шагов: 0.00\n",
      "Вероятность оказаться в состоянии (0) через 1 шагов: 0.00\n",
      "Вероятность оказаться в состоянии (1) через 1 шагов: 0.33\n",
      "Вероятность оказаться в состоянии (2) через 1 шагов: 0.33\n",
      "Вероятность оказаться в состоянии (3) через 1 шагов: 0.33\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Задайте еще несколько MDP:\n",
    "\n",
    "- C: <img src=https://i.ibb.co/Jj9LYHjP/mdp-c.png caption=\"C\" width=\"400\" height=\"100\">"
   ],
   "metadata": {
    "id": "p52R04np7Ku0"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:23.270374Z",
     "start_time": "2025-09-27T10:10:23.267305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "states = set(range(4))\n",
    "actions = set(range(2))\n",
    "\n",
    "print(f'{states=} | {actions=}')\n",
    "\n",
    "\n",
    "T = {  # (s, a)\n",
    "    (0, 0): 1,\n",
    "    (0, 1): 2,\n",
    "\n",
    "    (1, 0): 1,\n",
    "    (1, 1): 3,\n",
    "\n",
    "    (2, 0): 3,\n",
    "    (2, 1): 2,\n",
    "\n",
    "    (3, 0): 3,\n",
    "    (3, 1): 3\n",
    "}\n",
    "print(f'Transition function {T=}')\n",
    "\n",
    "A_mdp = states, actions, T"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states={0, 1, 2, 3} | actions={0, 1}\n",
      "Transition function T={(0, 0): 1, (0, 1): 2, (1, 0): 1, (1, 1): 3, (2, 0): 3, (2, 1): 2, (3, 0): 3, (3, 1): 3}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:23.321322Z",
     "start_time": "2025-09-27T10:10:23.318177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "P = get_transition_matrix(A_mdp)\n",
    "P"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:24.484506Z",
     "start_time": "2025-09-27T10:10:23.368987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k = 3\n",
    "for N in range(8):\n",
    "        print(f\"Вероятность оказаться в состоянии ({k}) через {N} шагов: {get_prob_of_state(A_mdp, P, 0, k, N):.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вероятность оказаться в состоянии (3) через 0 шагов: 0.00\n",
      "Вероятность оказаться в состоянии (3) через 1 шагов: 0.00\n",
      "Вероятность оказаться в состоянии (3) через 2 шагов: 0.50\n",
      "Вероятность оказаться в состоянии (3) через 3 шагов: 0.75\n",
      "Вероятность оказаться в состоянии (3) через 4 шагов: 0.88\n",
      "Вероятность оказаться в состоянии (3) через 5 шагов: 0.94\n",
      "Вероятность оказаться в состоянии (3) через 6 шагов: 0.97\n",
      "Вероятность оказаться в состоянии (3) через 7 шагов: 0.98\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Давайте попробуем задать двух агентов: случайного и оптимального (для каждой среды свой)."
   ],
   "metadata": {
    "id": "AiU5X4DH7TaG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Agent:\n",
    "    def __init__(self, actions):\n",
    "        self.rng = np.random.default_rng()\n",
    "        self.actions = np.array(list(actions))\n",
    "        \n",
    "        self.policy = None\n",
    "    \n",
    "    def set_policy(self, policy):\n",
    "        self.policy = policy\n",
    "\n",
    "    def act(self, state, strategy=\"random\"):\n",
    "        if strategy == \"random\":\n",
    "            return int(self.rng.integers(len(self.actions)))\n",
    "        return self.policy[state]"
   ],
   "metadata": {
    "id": "It2waXJi7WWN",
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:24.492990Z",
     "start_time": "2025-09-27T10:10:24.491084Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "В качестве дополнения, запишите стратегию агента\n",
    "\n",
    "Сделаю для $C$, в предположении, что хотим как можно быстрее добраться до терминальной точки."
   ],
   "metadata": {
    "id": "VH6uo4EP7ZqB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "optimal_policy_C = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 0\n",
    "}"
   ],
   "metadata": {
    "id": "t68c_r-W7asH",
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:24.536124Z",
     "start_time": "2025-09-27T10:10:24.534324Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:24.583714Z",
     "start_time": "2025-09-27T10:10:24.580465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = Agent(actions)\n",
    "agent.set_policy(optimal_policy_C)\n",
    "print(agent.act(1, \"random\"))\n",
    "print(agent.act(1, \"random\"))\n",
    "print(agent.act(1, \"random\"))\n",
    "print(agent.act(1, \"random\"))\n",
    "print(agent.act(1, \"random\"))\n",
    "print(agent.act(1, \"random\"))\n",
    "print(agent.act(1, \"optimal\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reward, reward function\n",
    "\n",
    "Теперь добавим произвольную функцию вознаграждения. Например, для A:\n",
    "\n",
    " *Я для C переделаю*"
   ],
   "metadata": {
    "id": "XGx2-KeH7lL4"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- C: <img src=https://i.ibb.co/Jj9LYHjP/mdp-c.png caption=\"C\" width=\"400\" height=\"100\">"
  },
  {
   "cell_type": "code",
   "source": [
    "R = {  # (s, a)\n",
    "    (0, 0): -10.0,\n",
    "    (0, 1): -1.0,\n",
    "\n",
    "    (1, 0): -1.0,\n",
    "    (1, 1): +100.0,\n",
    "\n",
    "    (2, 0): +10.0,\n",
    "    (2, 1): -1.0,\n",
    "\n",
    "    (3, 0): 0,\n",
    "    (3, 1): 0\n",
    "}\n",
    "print(R)\n",
    "\n",
    "A_mdp = *A_mdp, R\n",
    "print(A_mdp)"
   ],
   "metadata": {
    "id": "fk7umEnA7oFv",
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:24.635719Z",
     "start_time": "2025-09-27T10:10:24.633558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): -10.0, (0, 1): -1.0, (1, 0): -1.0, (1, 1): 100.0, (2, 0): 10.0, (2, 1): -1.0, (3, 0): 0, (3, 1): 0}\n",
      "({0, 1, 2, 3}, {0, 1}, {(0, 0): 1, (0, 1): 2, (1, 0): 1, (1, 1): 3, (2, 0): 3, (2, 1): 2, (3, 0): 3, (3, 1): 3}, {(0, 0): -10.0, (0, 1): -1.0, (1, 0): -1.0, (1, 1): 100.0, (2, 0): 10.0, (2, 1): -1.0, (3, 0): 0, (3, 1): 0})\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interaction loop, trajectory, termination, truncation, episode\n",
    "\n",
    "Общий цикл взаимодействия в рамках эпизода:\n",
    "1. Инициализировать среду: $s \\leftarrow \\text{env.init()}$\n",
    "2. Цикл:\n",
    "    - выбрать действие: $a \\leftarrow \\pi(s)$\n",
    "    - получить ответ от среды: $s, r, d \\leftarrow \\text{env.next(a)}$\n",
    "    - если $d == \\text{True}$, выйти из цикла"
   ],
   "metadata": {
    "id": "j92TZ1l67rVh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def run_episode(mdp):\n",
    "    states, actions, T, R = mdp\n",
    "    agent = Agent(actions)\n",
    "\n",
    "    s = 0\n",
    "    tau = []\n",
    "    for _ in range(5):\n",
    "        a = agent.act(s)\n",
    "        s_next = T[(s, a)]\n",
    "        r = R[(s, a)]\n",
    "\n",
    "        tau.append((s, a, r))\n",
    "        s = s_next\n",
    "\n",
    "    return tau\n",
    "\n",
    "run_episode(A_mdp)"
   ],
   "metadata": {
    "id": "MRPZACJt7vG8",
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:24.685968Z",
     "start_time": "2025-09-27T10:10:24.683283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, -1.0), (2, 0, 10.0), (3, 1, 0), (3, 0, 0), (3, 0, 0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "Termination — означает окончание эпизода, когда достигнуто терминальное состояние. Является частью задания среды.\n",
    "\n",
    "Truncation — означает окончание эпизода, когда достигнут лимит по числу шагов (=времени). Обычно является внешне заданным параметром для удобства обучения.\n",
    "\n",
    "Пока не будем вводить truncation, но поддержим termination: расширьте определение среды информацией о терминальных состояниях для всех описанных ранее сред. Сгенерируйте по несколько случайных траекторий для каждой среды."
   ],
   "metadata": {
    "id": "wbGEr8kl7xnS"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:24.734902Z",
     "start_time": "2025-09-27T10:10:24.733294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "terminal_states = {3,}\n",
    "states, actions, T, R = A_mdp\n",
    "A_mdp = states, terminal_states, actions, T, R"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:24.782277Z",
     "start_time": "2025-09-27T10:10:24.779502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_episode_with_policy(mdp, strategy=\"random\", policy=None, truncation=10):\n",
    "    states, terminal_states, actions, T, R = mdp\n",
    "    agent = Agent(actions)\n",
    "    if strategy != \"random\":\n",
    "        agent.set_policy(policy)\n",
    "\n",
    "    s = 0\n",
    "    tau = []\n",
    "    for _ in range(truncation):\n",
    "        a = agent.act(s, strategy)\n",
    "        s_next = T[(s, a)]\n",
    "        r = R[(s, a)]\n",
    "\n",
    "        tau.append((s, a, r))\n",
    "        s = s_next\n",
    "        \n",
    "        if s in terminal_states:\n",
    "            break\n",
    "\n",
    "    return tau"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Сгенерируем рандомные траектории:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:24.829610Z",
     "start_time": "2025-09-27T10:10:24.826940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "random_trajectories = []\n",
    "for _ in range(10):\n",
    "    tau = run_episode_with_policy(A_mdp, strategy=\"random\", truncation=10)\n",
    "    random_trajectories.append(tau)\n",
    "    print(tau)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1, -1.0), (2, 1, -1.0), (2, 0, 10.0)]\n",
      "[(0, 0, -10.0), (1, 1, 100.0)]\n",
      "[(0, 0, -10.0), (1, 1, 100.0)]\n",
      "[(0, 1, -1.0), (2, 1, -1.0), (2, 1, -1.0), (2, 1, -1.0), (2, 0, 10.0)]\n",
      "[(0, 1, -1.0), (2, 1, -1.0), (2, 1, -1.0), (2, 0, 10.0)]\n",
      "[(0, 0, -10.0), (1, 0, -1.0), (1, 0, -1.0), (1, 0, -1.0), (1, 0, -1.0), (1, 1, 100.0)]\n",
      "[(0, 0, -10.0), (1, 1, 100.0)]\n",
      "[(0, 0, -10.0), (1, 1, 100.0)]\n",
      "[(0, 1, -1.0), (2, 1, -1.0), (2, 1, -1.0), (2, 1, -1.0), (2, 0, 10.0)]\n",
      "[(0, 0, -10.0), (1, 0, -1.0), (1, 1, 100.0)]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Оптимальная траектория:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:24.878083Z",
     "start_time": "2025-09-27T10:10:24.876009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimal_trajectory = run_episode_with_policy(A_mdp, strategy=\"optimal\", policy=optimal_policy_C, truncation=10)\n",
    "optimal_trajectory"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, -10.0), (1, 1, 100.0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Return, expected return\n",
    "\n",
    "Наиболее важная метрика оценки качества работы агента: отдача.\n",
    "\n",
    "Отдача: $G(s_t) = \\sum_{i=t}^T r_i$\n",
    "\n",
    "Обычно также вводят параметр $\\gamma \\in [0, 1]$, дисконтирующий будущие вознаграждения. А еще, тк отдача может меняться от запуска к запуску благодаря вероятностным процессам, нас интересует отдача в среднем — ожидаемая отдача:\n",
    "\n",
    "$$\\hat{G}(s_t) = \\mathbb{E} [ \\sum_{i=t}^T \\gamma^{i-t} r_i ]$$\n",
    "\n",
    "Именно ее и оптимизируют в RL.\n",
    "\n",
    "Давайте научимся считать отдачу для состояний по траектории и считать среднюю отдачу."
   ],
   "metadata": {
    "id": "lxHNM9kS74WW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_returns(states, trajectories, gamma):\n",
    "    zeros = [0 for _ in range(len(states))]\n",
    "    returns = dict(zip(states, zeros))\n",
    "    counts  = dict(zip(states, zeros))\n",
    "    for tau in trajectories:\n",
    "        for i, (s, a, r) in enumerate(tau):\n",
    "            returns[s] += r * gamma ** i\n",
    "            counts[s]  += 1\n",
    "\n",
    "    def get_return(s):\n",
    "        return returns[s] / max(1, counts[s])\n",
    "    \n",
    "    return dict(zip(states, list(map(get_return, states))))"
   ],
   "metadata": {
    "id": "scdThsNA8T2B",
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:24.927734Z",
     "start_time": "2025-09-27T10:10:24.925333Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:24.973276Z",
     "start_time": "2025-09-27T10:10:24.971760Z"
    }
   },
   "cell_type": "code",
   "source": "gamma = 0.8",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:10:25.019248Z",
     "start_time": "2025-09-27T10:10:25.016934Z"
    }
   },
   "cell_type": "code",
   "source": "get_returns(states, random_trajectories, gamma)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: -6.4, 1: 37.600581818181816, 2: 1.0436923076923081, 3: 0.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  }
 ]
}
