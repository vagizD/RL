{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQDSU_zXGDAu"
      },
      "source": [
        "# Обучение с использованием модели и внутренней мотивации на основе ошибки модели\n",
        "\n",
        "В данном семинаре рассмотрим вариации модельного обучения на примере табличных алгоритмов. План будет следующий. Вначале мы оттолкнемся от классического алгоритма Dyna-Q — модификации Q-обучения, использующей идеальную табличную модель среды. Данный алгоритм работает с детерминированными средами, а значит обучение модели равносильно запоминанию переходов (т.к. каждый переход можно выучить за один шаг обучения).\n",
        "\n",
        "После реализации Dyna-Q представим, что модель среды обучается не моментально и может содержать ошибки. Моделируя среду таким образом, мы покроем одновременно два случая: стохастическая среда (распределения переходов придется учить даже в табличном случае) и нейросетевая апроксимация модели (тоже требует постепенного обучения на множестве примеров переходов). Посмотрим для этого варианта, насколько сильно неидеальность обучаемой модели нивелирует преимущества модельного обучения.\n",
        "\n",
        "Далее посмотрим, как можно построить сигнал внутренней мотивации на основе обучаемой модели и использовать его для ускорения обучения модели и исследования агентом среды. Также попробуем вариацию стратегии генерации воображаемых переходов для обучения стратегии — будем сэмплировать из модели не отдельные случайные переходы (как это сделано в Dyna-Q), а целые воображаемые траектории по аналогии с методами типа Dreamer. Причем эти траектории будут стартовать с текущего состояния агента, имитируя локальное планирование поведения с целью локального доуточнения Q-функции и стратегии по аналогии с методами на базе MCTS (Monte-Carlo Tree Search)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3Avl4qyaafn"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB1fDsxsGDAu"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "from collections import deque\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHzZlA3GGDAu"
      },
      "outputs": [],
      "source": [
        "def show_progress(avg_returns):\n",
        "    \"\"\"\n",
        "    Удобная функция, которая отображает прогресс обучения.\n",
        "    \"\"\"\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=[12, 4])\n",
        "    plt.subplot(1, 1, 1)\n",
        "    plt.plot(*zip(*avg_returns), label='Mean return')\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "def compare_logs(logs):\n",
        "    \"\"\"Функция сравнения кривых обучения\"\"\"\n",
        "    plt.figure(figsize=[12, 6])\n",
        "    for log, method_name in logs:\n",
        "        plt.plot(*zip(*log), label=f'{method_name}')\n",
        "        plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZIyWiSeGDAu"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
        "env.reset(seed=13)\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwrhpIzXGDAu"
      },
      "source": [
        "Для начала реализуем класс `Model` обучаемой идеальной модели мира будущего Dyna-Q агента. Некоторые аспекты модели сейчас могут показаться излишними — они нам пригодятся позднее."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3 балла)"
      ],
      "metadata": {
        "id": "Xea475F8bCMn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYaT_zZkGDAu"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, n_states, n_actions, seed):\n",
        "        # проинициализируйте атрибуты, необходимые вам для хранения\n",
        "        # модели (переходов и вознаграждения)\n",
        "        # например, можно хранить [в виде таблицы] табличные функции\n",
        "        #        r: (s, a) -> r\n",
        "        #   next_s: (s, a) -> next_s\n",
        "        # нужно так же дополнительно держать таблицы масок\n",
        "        # для посещенных состояний s `mask_state`\n",
        "        # и пар (s, a) `mask_state_action`\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "        self._rng = np.random.default_rng(seed)\n",
        "\n",
        "    def add(self, s: int, a: int, r: float, next_s: int) -> float:\n",
        "        # реализуйте мгновенное обучение модели\n",
        "        # для (s, a) -> r и (s, a) -> next_s\n",
        "        # не забудьте пометить посещения в масках\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "        # вернем также вознаграждение (понадобится позже)\n",
        "        return r\n",
        "\n",
        "    def sample_state(self) -> int:\n",
        "        # просэмплируйте и верните одно посещенное состояние\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "        return s\n",
        "\n",
        "    def sample_action(self, s) -> int:\n",
        "        # просэмплируйте и верните одно совершенное ранее действие\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "        return a\n",
        "\n",
        "    def predict_transition(self, s, a) -> tuple[float, int, float]:\n",
        "        # реализуйте [идеальное] предсказание моделей\n",
        "        # перехода и вознаграждения\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "        # дополнительно вернем степень уверенности в предсказании\n",
        "        # в текущем случае она равна значению в маске (=1)\n",
        "        confidence = self.mask_state_action[s, a]\n",
        "        return self.r[s, a], next_s, confidence\n",
        "\n",
        "    def sample(self) -> tuple[int, int, float, int, float]:\n",
        "        # реализуйте выбор одного случайного перехода\n",
        "        # и верните его вместе с уверенностью в предсказании\n",
        "        # (s, a, r, next_s, confidence)\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQh9Sl5iaafq"
      },
      "source": [
        "Теперь создадим класс `DynaQAgent`, который будет решать задачу обучения методом Dyna-Q, согласно приведенному на рисунке алгоритму:\n",
        "\n",
        "![dyna](https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/10/dyna.png)\n",
        "\n",
        "Добавим в реализации возможность выбора исследовательской стратегии между $\\epsilon$-жадной и softmax-стратегией. Выбор будет осуществляться на основе передаваемых в конструктор параметров: если передан `eps`, значит первый вариант, если `temp` (температура softmax), то второй."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2 балла)"
      ],
      "metadata": {
        "id": "TbvAFthObTCw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KvjR2wCGDAu"
      },
      "outputs": [],
      "source": [
        "def softmax(x, temp: float = 1.0):\n",
        "    e_x = np.exp((x - np.max(x)) / temp)\n",
        "    return e_x / np.sum(e_x)\n",
        "\n",
        "class DynaQAgent:\n",
        "    def __init__(\n",
        "        self, n_states, n_actions, lr, gamma, f_model, seed,\n",
        "        *, eps=None, temp=None\n",
        "    ):\n",
        "        self.Q = np.zeros((n_states, n_actions))\n",
        "        self.model = f_model(n_states, n_actions, seed=seed)\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.eps = eps\n",
        "        self.temp = temp\n",
        "        self.n_actions = n_actions\n",
        "        self._rng = np.random.default_rng(seed)\n",
        "\n",
        "    def act(self, s, greedy=False):\n",
        "        # выбираем действие, используя eps-greedy или softmax\n",
        "        # исследование среды\n",
        "        softmax_expl = self.temp is not None\n",
        "\n",
        "        if softmax_expl and not greedy:\n",
        "            action = self._rng.choice(\n",
        "                self.n_actions, p=softmax(self.Q[s], temp=self.temp)\n",
        "            )\n",
        "        elif (\n",
        "            not softmax_expl and not greedy\n",
        "            and self._rng.random() < self.eps\n",
        "        ):\n",
        "            action = self._rng.choice(self.n_actions)\n",
        "        else:\n",
        "            action = np.argmax(self.Q[s])  # используем Q-функцию\n",
        "        return action\n",
        "\n",
        "    def update(self, s, a, r, s_n, terminated, update_model: bool):\n",
        "        if update_model:\n",
        "            r = self.update_model(s, a, r, s_n)\n",
        "\n",
        "        # реализуйте шаг Q-обучения\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "\n",
        "    def update_model(self, s, a, r, s_n):\n",
        "        return self.model.add(s, a, r, s_n)\n",
        "\n",
        "    def dream(self, max_steps, **_):\n",
        "        # Совершает шаги обучения на `max_steps`\n",
        "        # сгенрированных моделью переходах\n",
        "        for _ in range(max_steps):\n",
        "            # Добавьте шаг обучения с использованием модели на одном\n",
        "            # сгенерированном моделью переходе\n",
        "            # NB: будем использовать значение confidence от модели\n",
        "            # как вероятность использовать сгенерированный переход\n",
        "            # для шага обучения\n",
        "            # NB2: флаг `terminated` будем передавать False\n",
        "            ####### Здесь ваш код ########\n",
        "            raise NotImplementedError\n",
        "            ##############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMgDoMXqaafs"
      },
      "outputs": [],
      "source": [
        "def train_episode(rng, env, agent, on_model_updates, is_eval=False):\n",
        "    \"\"\"Провести один эпизод обучения или тестирования.\"\"\"\n",
        "    state, _ = env.reset(seed=int(rng.integers(1_000_000)))\n",
        "    episode_return = 0.0\n",
        "    while True:\n",
        "        action = agent.act(state, greedy=is_eval)\n",
        "\n",
        "        # выполняем действие в среде\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        if not is_eval:\n",
        "            # шаг обучения\n",
        "            agent.update(\n",
        "                state, action, reward, next_state, terminated,\n",
        "                update_model=True\n",
        "            )\n",
        "        state = next_state\n",
        "        episode_return += reward\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "        if not is_eval:\n",
        "            # шаги обучения по модельным данным\n",
        "            agent.dream(on_model_updates, state=state)\n",
        "    return episode_return\n",
        "\n",
        "def train(\n",
        "    env, agent, n_episodes, on_model_updates, seed,\n",
        "    show_progress_schedule=100, show_model_progress=False\n",
        "):\n",
        "    train_progress = []\n",
        "    returns_batch = deque(maxlen=4*show_progress_schedule)\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    for i in range(1, n_episodes+1):\n",
        "        # один эпизод обучения\n",
        "        train_episode(rng, env, agent, on_model_updates)\n",
        "        # и один эпизод тестирования\n",
        "        returns_batch.append(\n",
        "            train_episode(rng, env, agent, on_model_updates, is_eval=True)\n",
        "        )\n",
        "\n",
        "        if i % show_progress_schedule == 0 and not show_model_progress:\n",
        "            # график прогресса: отдача за эпизод\n",
        "            train_progress.append(\n",
        "                (i, np.mean(returns_batch))\n",
        "            )\n",
        "            show_progress(train_progress)\n",
        "            print(\n",
        "                f\"Episode: {i}, Return: {returns_batch[-1]}, \"\n",
        "                f\"AvgReturn[{show_progress_schedule}]: {train_progress[-1][1]:.0f}\"\n",
        "            )\n",
        "        if i % show_progress_schedule == 0 and show_model_progress:\n",
        "            # график прогресса: доля выученности посещенных состояний среды\n",
        "            visited_mask = agent.model.mask_state_action > 0\n",
        "            avg_model_knowledge = np.mean(\n",
        "                agent.model.mask_state_action[visited_mask]\n",
        "            )\n",
        "            train_progress.append(\n",
        "                (i, avg_model_knowledge)\n",
        "            )\n",
        "            show_progress(train_progress)\n",
        "\n",
        "    return train_progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yilYQi2aafs"
      },
      "source": [
        "### Q-learning\n",
        "\n",
        "Обратите внимание, параметр `on_model_updates`, отвечающий на каждом шаге в среде за число дополнительно сгенерированного моделью опыта, равен 0 $\\Rightarrow$ значит мы запускаем обычный Q-learning и модель хоть и учим, но ей не пользуемся"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMP_VAJ0GDAu"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=100)\n",
        "seed = 1337\n",
        "agent = DynaQAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.1,\n",
        "    gamma=0.975, temp=0.25, seed=seed,\n",
        "    f_model=Model\n",
        ")\n",
        "\n",
        "log_q = train(env, agent, n_episodes=4000, on_model_updates=0, seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1 балл)"
      ],
      "metadata": {
        "id": "I0YpLpqaOsdl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORx9eeOPaaft"
      },
      "source": [
        "### DynaQ\n",
        "\n",
        "1. Сравните скорость обучения алгоритмов Q-обучение и Dyna-Q с параметром `on_model_updates` равным 10.\n",
        "2. Сравните скорость обучения Dyna-Q при различных `on_model_updates`. В каком случае получились лучшие результаты?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9oH69-Waaft"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=100)\n",
        "seed = 1337\n",
        "agent = DynaQAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.1,\n",
        "    gamma=0.975, temp=0.25, seed=seed,\n",
        "    f_model=Model\n",
        ")\n",
        "log_dyna_q = train(env, agent, n_episodes=4000, on_model_updates=5, seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioKDIytGaafu"
      },
      "source": [
        "## Dyna-Q с неидеальной моделью мира\n",
        "\n",
        "Давайте теперь представим, что мы ничего не знаем о среде. В частности, мы будем даже не уверены в том, что она детерминированная. Будем именно \"учить\" переходы с некоторой скоростью обучения (параметром `lr` $\\in (0, 1)$). Сделаем это следующим образом. Будем продолжать использовать знание о том, что среда такси детерминированная, а значит будем продолжать хранить детерминированную матрицу переходов: $(s, a) \\rightarrow s'$. Но если раньше мы также хранили **бинарную** маску \"выученности\" перехода для пар (s,a): $(s, a) \\rightarrow 0 | 1$, то теперь будем хранить **степень** выученности $\\in [0, 1]$, где 0 означает полное незнание, а 1 - полное знание. Таким образом каждый шаг обучения модели будем повышать это значение, устремляя его к 1.\n",
        "\n",
        "Будем интерпретировать степень выученности перехода как уверенность в предсказании и использовать как вероятность дать правильное предсказание перехода данной моделью $P[s_{t+1} |s_t,a_t]$. Тогда в случае планирования будущего моделью, она будет с этой вероятностью давать правильный ответ, а с вероятностью $1 - P[s_{t+1} |s_t,a_t]$ будет давать случайное предсказание.\n",
        "\n",
        "*NB: можно было бы использовать в качестве аппроксиматора нейронную сеть, но мы сознательно останемся в рамках табличных моделей — именно обучение аппроксиматора и будем сейчас эмулировать, но таким образом, чтобы нам было проще контролировать скорость обучения каждого перехода.*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2 балла)"
      ],
      "metadata": {
        "id": "_ZJI_PKPbkxr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "muvLIv9raafu"
      },
      "outputs": [],
      "source": [
        "class ImperfectModel(Model):\n",
        "    # обратите внимание, мы наследуемся и переопределяем нужные\n",
        "    # методы оригинальной идеальной модели\n",
        "\n",
        "    def __init__(\n",
        "        self, n_states, n_actions, lr: float, seed: int,\n",
        "        n_updates: int = 4, buffer_size: int = 1000\n",
        "    ):\n",
        "        super().__init__(n_states, n_actions, seed)\n",
        "        # адаптируйте структуру модели так, чтобы переход (s, a) -> next_s\n",
        "        # обучался со скоростью `lr`.\n",
        "        # Для этого поменяем смысл маски посещенных пар (s, a).\n",
        "        # Заменим бинарную маску на вещественнозначную с числами из [0, 1].\n",
        "        # Будем интерпретировать их как степень выученности переходов\n",
        "        # или нашей уверенности в их предсказании\n",
        "        self.lr = lr\n",
        "        self.mask_state_action = np.zeros((n_states, n_actions), dtype=float)\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "        self.n_updates = n_updates\n",
        "\n",
        "    def add(self, s: int, a: int, r: float, next_s: int) -> float:\n",
        "        # Шаг 1: обновим и положим новый опыт в буфер\n",
        "        new_transition = s, a, r, next_s\n",
        "        self._update(*new_transition)\n",
        "        self.buffer.append(new_transition)\n",
        "\n",
        "        # Шаг 2: сделаем n_updates итераций обучения модели\n",
        "        # на данных из буфера\n",
        "        sz_buffer = len(self.buffer)\n",
        "        for _ in range(min(self.n_updates, sz_buffer)):\n",
        "            ix = self._rng.integers(sz_buffer)\n",
        "            self._update(*self.buffer[ix])\n",
        "        # вернем входное вознаграждение без изменений\n",
        "        return r\n",
        "\n",
        "    def _update(self, s: int, a: int, r: float, next_s: int):\n",
        "        # сохраняем уверенность в предсказании\n",
        "        confidence = self.mask_state_action[s][a]\n",
        "        # воспользуемся шагом обучения идеальной модели\n",
        "        # NB: она перезатрет self.mask_state_action[s][a]\n",
        "        super().add(s, a, r, next_s)\n",
        "\n",
        "        # Сделаем шаг обучения уверенности в предсказании\n",
        "        # перехода со скоростью `lr`.\n",
        "        # Для простоты, функцию вознаграждения обучаем мгновенно\n",
        "        # В идеальной модели было:\n",
        "        # self.mask_state_action[s][a] = 1\n",
        "        # Теперь будет:\n",
        "        self.mask_state_action[s][a] = confidence + self.lr * (1 - confidence)\n",
        "\n",
        "    def predict_transition(self, s, a) -> tuple[float, int, float]:\n",
        "        # Реализуйте неидеальное предсказание модели перехода:\n",
        "        # Используйте величину уверенности как вероятность правильно\n",
        "        # предсказать переход. Иначе будем выдавать случайное предсказание\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "        return self.r[s, a], next_s, confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj4FGlzMaafu"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=100)\n",
        "seed = 1337\n",
        "agent = DynaQAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.1,\n",
        "    gamma=0.975, temp=0.25, seed=seed,\n",
        "    f_model=partial(ImperfectModel, lr=0.02, buffer_size=10000, n_updates=5)\n",
        ")\n",
        "\n",
        "log_dyna_q_imp = train(\n",
        "    env, agent, n_episodes=4000, on_model_updates=5, seed=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45i0V4C3aafv"
      },
      "source": [
        "**Вопрос на подумать**: Что можно сказать о результатах модельного обучения в случае, когда модель неидеальна?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "iZamoyzkaafv"
      },
      "source": [
        "## Dyna-Q с неидеальной моделью и внутренней мотивацией на основе ошибки модели\n",
        "\n",
        "Сигнал ошибки модели является хорошим сигналом мотивации к исследованию, \"отправляя\" агента в недоисследованные зоны пространства состояний. Как следствие два плюса: а) более быстрое и целенаправленное обучение самой модели среды (которая дает бонус к скорости обучения через обучение в воображении) и б) более целенаправленное исследование самой среды, которое поможет агенту быстрее найти состояния с высоким внешним вознаграждением.\n",
        "\n",
        "Давайте посмотрим, так ли это. Реализуйте аналогичную предыдущему пункту \"неидеальную\" внутреннюю модель среды агента, но теперь также добавьте в нее внутреннюю мотивацию. Для этого можно, например, добавлять ошибку предсказания на основе вероятности дать случайное предсказание $1 - P[s_{t+1} |s_t,a_t]$ к полученному внешнему вознаграждению $r_{t+1}$ при обучении функции вознаграждения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "d_jNueGmaafv"
      },
      "outputs": [],
      "source": [
        "class IntrinsicMotivationModel(ImperfectModel):\n",
        "    def __init__(\n",
        "        self, n_states, n_actions, lr: float, seed: int,\n",
        "        n_updates: int = 4, buffer_size: int = 1000,\n",
        "        add_intrinsic: float=0.\n",
        "    ):\n",
        "        super().__init__(\n",
        "            n_states, n_actions, lr, seed,\n",
        "            n_updates=n_updates, buffer_size=buffer_size\n",
        "        )\n",
        "        self.add_intrinsic = add_intrinsic\n",
        "\n",
        "    def add(self, s: int, a: int, r: float, next_s: int) -> float:\n",
        "        super().add(s, a, r, next_s)\n",
        "\n",
        "        # вернем вознаграждение реального перехода\n",
        "        # с добавкой внутренней мотивации\n",
        "        r += self._get_intrinsic_reward(s, a)\n",
        "        return r\n",
        "\n",
        "    def sample(self) -> tuple[int, int, float, int, float]:\n",
        "        s, a, r, next_s, confidence = super().sample()\n",
        "\n",
        "        # вернем вознаграждение предсказанного перехода\n",
        "        # с добавкой внутренней мотивации\n",
        "        r += self._get_intrinsic_reward(s, a)\n",
        "        return s, a, r, next_s, confidence\n",
        "\n",
        "    def _get_intrinsic_reward(self, s, a):\n",
        "        # вычислите значение сигнала внутренней мотивации r_im\n",
        "        # можете адаптировать код ниже.\n",
        "        r_im = 0.0\n",
        "        confidence = self.mask_state_action[s][a]\n",
        "        if self.add_intrinsic > 0.:\n",
        "            r_im = 1.0 - confidence**3\n",
        "        return self.add_intrinsic * r_im"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-BNvuT9aafv"
      },
      "source": [
        "Сначала проверим, что все запускается и работает: масштабирующий коэффициент добавочной внутренней мотивации `add_intrinsic` равен 0 $\\Rightarrow$ это обычное модельное обучение с неидеальной моделью как в подпункте выше, просто теперь мы использовали объект модели нового класса. Графики должны совпадать"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwWQ5XGYaafv"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=100)\n",
        "seed = 1337\n",
        "agent = DynaQAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.1,\n",
        "    gamma=0.975, temp=0.25, seed=seed,\n",
        "    f_model=partial(\n",
        "        IntrinsicMotivationModel, lr=0.02, buffer_size=10000, n_updates=5,\n",
        "        add_intrinsic=0.\n",
        "    )\n",
        ")\n",
        "\n",
        "log_dyna_q_imp_im = train(\n",
        "    env, agent, n_episodes=4000, on_model_updates=5, seed=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehp_SZtzaafv"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# а теперь со включенной внутренней мотивацией (см add_intrinsic параметр)\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=100)\n",
        "seed = 1337\n",
        "agent = DynaQAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.1,\n",
        "    gamma=0.975, temp=0.25, seed=seed,\n",
        "    f_model=partial(\n",
        "        IntrinsicMotivationModel, lr=0.025, buffer_size=10000, n_updates=5,\n",
        "        add_intrinsic=0.1\n",
        "    )\n",
        ")\n",
        "\n",
        "log_dyna_q_imp_im = train(\n",
        "    env, agent, n_episodes=4000, on_model_updates=5, seed=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "MdHR7rCGaafv"
      },
      "source": [
        "## With imaginary trajectories\n",
        "До этого процесс обучения по модельным данным происходил беспорядочно — агент случайно сэмплировал состояние, которое ему знакомо, выбирал случайно действие, которое однажды уже выбирал, предсказывал переход (следующее состояние и вознаграждение) и обучался на одном переходе. Процесс повторялся в течение нескольких независимых шагов.\n",
        "\n",
        "Теперь давайте попробуем воспроизвести процесс обучения в воображении, похожий на планирование из текущей позиции — чтобы агент воображал целую траекторию, начинающуюся с текущего состояния (то есть додумывал/планировал возможное продолжение событий, начиная с текущего момента)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2 балла)"
      ],
      "metadata": {
        "id": "z__7Szk0bxtF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcbvNikaaafw"
      },
      "outputs": [],
      "source": [
        "class DynaQDreamingAgent(DynaQAgent):\n",
        "    def dream(self, max_steps, state):\n",
        "        # текущее состояние — стартовое для траектории длины max_steps в воображении\n",
        "        s = state\n",
        "        for _ in range(max_steps):\n",
        "            # добавьте последовательный шаг в воображении с шагом обучения\n",
        "            # Действие теперь будем выбирать на основе\n",
        "            # исследовательской стратегии агента\n",
        "            # Будем прерывать генерацию траектории на основе\n",
        "            # уверенности перехода с вероятностью `1 - confidence`\n",
        "            ####### Здесь ваш код ########\n",
        "            raise NotImplementedError\n",
        "            ##############################\n",
        "            s = next_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p7lYaNTaafw"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# сначала без внутренней мотивации\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=100)\n",
        "seed = 1337\n",
        "agent = DynaQDreamingAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.1,\n",
        "    gamma=0.975, temp=0.25, seed=seed,\n",
        "    f_model=partial(\n",
        "        IntrinsicMotivationModel, lr=0.025, buffer_size=10000, n_updates=5,\n",
        "        add_intrinsic=0.\n",
        "    )\n",
        ")\n",
        "\n",
        "log_dyna_q_imp_dreamer = train(\n",
        "    env, agent, n_episodes=4000, on_model_updates=5, seed=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ6W-Ptuaafw"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# и то же самое с внутренней мотивацией\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=100)\n",
        "seed = 1337\n",
        "agent = DynaQDreamingAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.1,\n",
        "    gamma=0.975, temp=0.25, seed=seed,\n",
        "    f_model=partial(\n",
        "        IntrinsicMotivationModel, lr=0.025, buffer_size=10000, n_updates=5,\n",
        "        add_intrinsic=0.1\n",
        "    )\n",
        ")\n",
        "\n",
        "log_dyna_q_imp_im_dreamer = train(\n",
        "    env, agent, n_episodes=4000, on_model_updates=4, seed=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "7KIq-sBsaafw"
      },
      "source": [
        "## Сравнение подходов\n",
        "Теперь просимулируем задачу посложнее и сравним результаты. В качестве усложнения выбраны:\n",
        "\n",
        "- более жесткое ограничение на длину эпизода max_episode_steps = 50\n",
        "- меньшая скорость обучения lr Q-функции и модели\n",
        "- более жадная стратегия исследования (ниже температура софтмакса)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhiGhdjDaafw"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=50)\n",
        "seed = 42\n",
        "agent = DynaQAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.03,\n",
        "    gamma=0.98, temp=0.1, seed=seed,\n",
        "    f_model=Model\n",
        ")\n",
        "\n",
        "log_q = train(\n",
        "    env, agent, n_episodes=15000, on_model_updates=0, seed=seed,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yskspOWaafw"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=50)\n",
        "seed = 42\n",
        "agent = DynaQAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.03,\n",
        "    gamma=0.98, temp=0.1, seed=seed,\n",
        "    f_model=Model\n",
        ")\n",
        "log_dyna_q = train(env, agent, n_episodes=8000, on_model_updates=5, seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDAXfpsQaafx"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=50)\n",
        "seed = 42\n",
        "agent = DynaQAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.03,\n",
        "    gamma=0.98, temp=0.1, seed=seed,\n",
        "    f_model=partial(ImperfectModel, lr=0.004, buffer_size=10000, n_updates=5)\n",
        ")\n",
        "\n",
        "log_dyna_q_imp = train(\n",
        "    env, agent, n_episodes=15000, on_model_updates=5, seed=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kp4qHr_aafx"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=50)\n",
        "seed = 42\n",
        "agent = DynaQAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.03,\n",
        "    gamma=0.98, temp=0.1, seed=seed,\n",
        "    f_model=partial(\n",
        "        IntrinsicMotivationModel, lr=0.004, buffer_size=10000, n_updates=5,\n",
        "        add_intrinsic=0.1\n",
        "    )\n",
        ")\n",
        "\n",
        "log_dyna_q_imp_im = train(\n",
        "    env, agent, n_episodes=10000, on_model_updates=5, seed=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuvWwUP3aafx"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=50)\n",
        "seed = 42\n",
        "agent = DynaQDreamingAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.03,\n",
        "    gamma=0.98, temp=0.1, seed=seed,\n",
        "    f_model=partial(\n",
        "        IntrinsicMotivationModel, lr=0.004, buffer_size=10000, n_updates=5, add_intrinsic=0.\n",
        "    )\n",
        ")\n",
        "\n",
        "log_dyna_q_imp_dreaming = train(\n",
        "    env, agent, n_episodes=8000, on_model_updates=5, seed=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFofMjfyaafx"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=50)\n",
        "seed = 42\n",
        "agent = DynaQDreamingAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.03,\n",
        "    gamma=0.98, temp=0.1, seed=seed,\n",
        "    f_model=partial(\n",
        "        IntrinsicMotivationModel, lr=0.004, buffer_size=10000, n_updates=5, add_intrinsic=0.1\n",
        "    )\n",
        ")\n",
        "\n",
        "log_dyna_q_imp_im_dreaming = train(\n",
        "    env, agent, n_episodes=8000, on_model_updates=5, seed=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlM9Da4yaafx"
      },
      "outputs": [],
      "source": [
        "compare_logs([\n",
        "    (log_q, 'Q-learning'),\n",
        "    (log_dyna_q, 'Dyna-Q'),\n",
        "    (log_dyna_q_imp, 'Dyna-Q imperfect model'),\n",
        "    (log_dyna_q_imp_im, 'Dyna-Q imperfect model + IM'),\n",
        "    (log_dyna_q_imp_dreaming, 'Dyna-Q imperfect model + Dreaming'),\n",
        "    (log_dyna_q_imp_im_dreaming, 'Dyna-Q imperfect model + IM + Dreaming'),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1-8txAgaafy"
      },
      "source": [
        "## Model learning progress\n",
        "\n",
        "Теперь давайте запустим те же самые эксперименты, но отрисуем средний прогресс обучения переходов в модели (0 — модель полностью необучена, 1 — модель полностью обучила все возможные переходы) и посмотрим, насколько велика разница в скорости исследования среды и обучения модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3l8d-Asaafy"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=50)\n",
        "seed = 42\n",
        "agent = DynaQAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.03,\n",
        "    gamma=0.98, temp=0.1, seed=seed,\n",
        "    f_model=partial(ImperfectModel, lr=0.004)\n",
        ")\n",
        "\n",
        "_log_dyna_q_imp = train(\n",
        "    env, agent, n_episodes=5000, on_model_updates=5, seed=seed,\n",
        "    show_model_progress=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRgMDSU2aafy"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=50)\n",
        "seed = 42\n",
        "agent = DynaQAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.03,\n",
        "    gamma=0.975, eps=0.02, seed=seed,\n",
        "    f_model=partial(\n",
        "        IntrinsicMotivationModel, lr=0.004, buffer_size=10000, n_updates=5,\n",
        "        add_intrinsic=0.1\n",
        "    )\n",
        ")\n",
        "\n",
        "_log_dyna_q_imp_im = train(\n",
        "    env, agent, n_episodes=5000, on_model_updates=5, seed=seed,\n",
        "    show_model_progress=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4Z0gPCNaafy"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=50)\n",
        "seed = 42\n",
        "agent = DynaQDreamingAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.03,\n",
        "    gamma=0.98, temp=0.1, seed=seed,\n",
        "    f_model=partial(\n",
        "        IntrinsicMotivationModel, lr=0.004, buffer_size=10000, n_updates=5,\n",
        "        add_intrinsic=0.\n",
        "    )\n",
        ")\n",
        "\n",
        "_log_dyna_q_imp_dreaming = train(\n",
        "    env, agent, n_episodes=5000, on_model_updates=5, seed=seed,\n",
        "    show_model_progress=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXt2afwQaaf3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "env = gym.make(\"Taxi-v3\", max_episode_steps=50)\n",
        "seed = 42\n",
        "agent = DynaQDreamingAgent(\n",
        "    env.observation_space.n, env.action_space.n, lr=0.03,\n",
        "    gamma=0.98, temp=0.1, seed=seed,\n",
        "    f_model=partial(\n",
        "        IntrinsicMotivationModel, lr=0.004, buffer_size=10000, n_updates=5,\n",
        "        add_intrinsic=0.1\n",
        "    )\n",
        ")\n",
        "\n",
        "_log_dyna_q_imp_im_dreaming = train(\n",
        "    env, agent, n_episodes=5000, on_model_updates=5, seed=seed,\n",
        "    show_model_progress=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQE_cn6Oaaf3"
      },
      "outputs": [],
      "source": [
        "compare_logs([\n",
        "    (_log_dyna_q_imp, 'Dyna-Q imperfect model'),\n",
        "    (_log_dyna_q_imp_im, 'Dyna-Q imperfect model + IM'),\n",
        "    (_log_dyna_q_imp_dreaming, 'Dyna-Q imperfect model + Dreaming'),\n",
        "    (_log_dyna_q_imp_im_dreaming, 'Dyna-Q imperfect model + IM + Dreaming'),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBKTT8tywUwS",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}