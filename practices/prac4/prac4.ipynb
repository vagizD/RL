{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "962eeb72-0836-4fb1-80cd-154f7d5a111f",
      "metadata": {
        "id": "962eeb72-0836-4fb1-80cd-154f7d5a111f"
      },
      "source": [
        "# DQN\n",
        "\n",
        "Метод обучения DQN — это нейросетевая адаптация алгоритма Q-learning. Также для него разработан набор дополнений, которые становятся актуальными при переходе к обучению глубоких нейронных сетей и решению более сложных задач (то есть задач с бОльшим пространством состояний).\n",
        "\n",
        "Реализуем алгоритм DQN для решения среды [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/), цель которой балансировать палочкой в вертикальном положении, управляя только тележкой, к которой она прикреплена. Будем использовать библиотеку PyTorch для обучения нейронной сети, аппроксимирующей Q-функцию (но вы можете воспользоваться и любой другой библиотекой для обучения глубоких сетей, таких как TensorFlow или Jax).\n",
        "\n",
        "![cartpole](https://gymnasium.farama.org/_images/cart_pole.gif)\n",
        "\n",
        "![cartpole](https://www.researchgate.net/publication/362568623/figure/fig5/AS:1187029731807278@1660021350587/Screen-capture-of-the-OpenAI-Gym-CartPole-problem-with-annotations-showing-the-cart.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c0762ee-f55c-4bec-b544-fb96bcdb5618",
      "metadata": {
        "id": "1c0762ee-f55c-4bec-b544-fb96bcdb5618"
      },
      "outputs": [],
      "source": [
        "# Cтавим нужные зависимости, если это колаб\n",
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3\n",
        "    !pip -q install setuptools==59.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa4a81a2-b188-47be-8b99-66a95401eed4",
      "metadata": {
        "id": "aa4a81a2-b188-47be-8b99-66a95401eed4"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "import base64\n",
        "import io\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# import cv2\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pygame\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.registration import WrapperSpec\n",
        "%matplotlib inline\n",
        "\n",
        "if COLAB:\n",
        "    from google.colab import files\n",
        "    from google.colab.patches import cv2_imshow\n",
        "    from google.colab import output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24bcf642-10cc-431b-b9b2-18bc2bfbf1a3",
      "metadata": {
        "id": "24bcf642-10cc-431b-b9b2-18bc2bfbf1a3"
      },
      "source": [
        "### Action Space\n",
        "\n",
        "The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction of the fixed force the cart is pushed with.\n",
        "\n",
        "- 0: Push cart to the left\n",
        "- 1: Push cart to the right\n",
        "\n",
        "**Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
        "the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
        "\n",
        "### Observation Space\n",
        "\n",
        "The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
        "\n",
        "| Num | Observation           | Min                 | Max               |\n",
        "|-----|-----------------------|---------------------|-------------------|\n",
        "| 0   | Cart Position         | -4.8                | 4.8               |\n",
        "| 1   | Cart Velocity         | -Inf                | Inf               |\n",
        "| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
        "| 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
        "\n",
        "**Note:** While the ranges above denote the possible values for observation space of each element, it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
        "\n",
        "- The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates if the cart leaves the `(-2.4, 2.4)` range.\n",
        "- The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
        "   if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
        "\n",
        "### Rewards\n",
        "\n",
        "Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\n",
        "including the termination step, is allotted. The threshold for rewards is 500 for v1 and 200 for v0.\n",
        "\n",
        "### Starting State\n",
        "\n",
        "All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
        "\n",
        "### Episode End\n",
        "\n",
        "The episode ends if any one of the following occurs:\n",
        "\n",
        "1. Termination: Pole Angle is greater than ±12°\n",
        "2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
        "3. Truncation: Episode length is greater than 500 (200 for v0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d8fea0d-7e29-4eea-888c-26e2bf9f16c4",
      "metadata": {
        "id": "9d8fea0d-7e29-4eea-888c-26e2bf9f16c4"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", max_episode_steps=1000)\n",
        "env.reset()\n",
        "\n",
        "# Выведем информацию о пространствах состояний и действий\n",
        "print(f'{env.observation_space=}')\n",
        "print(f'{env.action_space=}')\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "print(f'Action_space: {n_actions} | State_space: {state_dim}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "febc1aff-ab5e-4d9b-9c6d-a6e5b72d2381",
      "metadata": {
        "id": "febc1aff-ab5e-4d9b-9c6d-a6e5b72d2381"
      },
      "source": [
        "Т.к. описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
        "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2020/RL/figures/DQN.svg\">\n",
        "\n",
        "Будем использовать только полносвязные слои (``torch.nn.Linear``) и простые активационные функции (``torch.nn.ReLU``). Сигмоиды и другие похожие функции активации могут плохо работать с ненормализованными входными данными.\n",
        "\n",
        "Будем приближать Q-функцию агента, минимизируя среднеквадратичную TD-ошибку:\n",
        "$$\n",
        "\\delta = Q_{\\theta}(s, a) - [r(s, a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]\n",
        "$$\n",
        "$$\n",
        "L = \\frac{1}{N} \\sum_i \\delta_i^2,\n",
        "$$\n",
        "где\n",
        "* $s, a, r, s'$ состояние, действие, вознаграждение и следующее состояние\n",
        "* $\\gamma$ дисконтирующий множитель.\n",
        "\n",
        "Основная тонкость состоит в использовании $Q_{-}(s',a')$. Это та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. В научных статьях можно обнаружить следующее обозначение для остановки градиента: $SG(\\cdot)$. В PyTorch есть метод `.detach()` класса `Tensor`, который возвращает тензор с выключенными градиентами, а также контекстный менеджер `with torch.no_grad()`, который задает контекст с вычислениями, для которых не вычисляется градиент."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1 балл)"
      ],
      "metadata": {
        "id": "uYAQTcVOOV-t"
      },
      "id": "uYAQTcVOOV-t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f22be5c6-da87-46aa-95ec-ec792e46a355",
      "metadata": {
        "id": "f22be5c6-da87-46aa-95ec-ec792e46a355"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def create_network(input_dim, hidden_dims, output_dim):\n",
        "    # network = nn.Sequential(\n",
        "    #    torch.nn.Linear(input_dim, ...),\n",
        "    #    torch.nn.ReLU() or Tanh(),\n",
        "    #    ...\n",
        "    # )\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "    return network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25af58f1-46d1-4cf4-abf7-e6f62161595d",
      "metadata": {
        "id": "25af58f1-46d1-4cf4-abf7-e6f62161595d"
      },
      "source": [
        "Добавим $\\epsilon$-жадный выбор действий (1 балл):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1684ea0-11e4-4f16-a7d3-45c97dbcbc99",
      "metadata": {
        "id": "d1684ea0-11e4-4f16-a7d3-45c97dbcbc99"
      },
      "outputs": [],
      "source": [
        "def select_action_eps_greedy(Q, state, epsilon):\n",
        "    \"\"\"Выбирает действие epsilon-жадно.\"\"\"\n",
        "    if not isinstance(state, torch.Tensor):\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "    Q_s = Q(state).detach().numpy()\n",
        "\n",
        "    # action =\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    action = int(action)\n",
        "    return action\n",
        "\n",
        "\n",
        "Q = create_network(\n",
        "    input_dim=np.prod(state_dim), hidden_dims=[64, 64], output_dim=n_actions\n",
        ")\n",
        "select_action_eps_greedy(Q, env.reset()[0].flatten(), epsilon=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(4 балла)"
      ],
      "metadata": {
        "id": "tn4oKO1DOgFd"
      },
      "id": "tn4oKO1DOgFd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51cbe381-b616-4359-8ebc-ab8ca63302ef",
      "metadata": {
        "id": "51cbe381-b616-4359-8ebc-ab8ca63302ef"
      },
      "outputs": [],
      "source": [
        "def to_tensor(x, dtype=np.float32):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x\n",
        "    x = np.asarray(x, dtype=dtype)\n",
        "    x = torch.from_numpy(x)\n",
        "    return x\n",
        "\n",
        "def compute_td_target(\n",
        "        Q, rewards, next_states, terminateds, gamma=0.99, check_shapes=True,\n",
        "):\n",
        "    \"\"\" Считает TD-target.\"\"\"\n",
        "\n",
        "    # переводим входные данные в тензоры\n",
        "    r = to_tensor(rewards)  # shape: [batch_size]\n",
        "    s_next = to_tensor(next_states)  # shape: [batch_size, state_size]\n",
        "    term = to_tensor(terminateds, bool)  # shape: [batch_size]\n",
        "\n",
        "    # получаем Q[s_next, .] — значения полезности всех действий в следующем состоянии\n",
        "    # Q_sn = ...,\n",
        "    # а затем вычисляем V^*[s_next] — оптимальные значения полезности следующем состоянии\n",
        "    # V_sn = ...\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    assert V_sn.dtype == torch.float32\n",
        "\n",
        "    # вычисляем TD target\n",
        "    # target = ...\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    if check_shapes:\n",
        "        assert Q_sn.data.dim() == 2, \\\n",
        "            \"убедитесь, что вы предсказали q-значения для всех действий в следующем состоянии\"\n",
        "        assert V_sn.data.dim() == 1, \\\n",
        "            \"убедитесь, что вы вычислили V (s ') как максимум только по оси действий, а не по всем осям\"\n",
        "        assert target.data.dim() == 1, \\\n",
        "            \"что-то не так с целевыми q-значениями, они должны быть вектором\"\n",
        "\n",
        "    return target\n",
        "\n",
        "\n",
        "def compute_td_loss(\n",
        "        Q, states, actions, td_target, regularizer=.1, out_non_reduced_losses=False\n",
        "):\n",
        "    \"\"\" Считает TD ошибку.\"\"\"\n",
        "\n",
        "    # переводим входные данные в тензоры\n",
        "    s = to_tensor(states)  # shape: [batch_size, state_size]\n",
        "    a = to_tensor(actions, int).long()  # shape: [batch_size]\n",
        "\n",
        "    # получаем Q[s, a] для выбранных действий в текущих состояниях\n",
        "    # (для каждого примера из батча)\n",
        "    # Q_s_a = ...\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    # вычисляем TD error\n",
        "    # td_error = ...\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    # MSE loss для минимизации\n",
        "    td_losses = td_error ** 2\n",
        "    loss = torch.mean(td_losses)\n",
        "    # добавляем L1 регуляризацию на значения Q\n",
        "    loss += regularizer * torch.abs(Q_s_a).mean()\n",
        "\n",
        "    if out_non_reduced_losses:\n",
        "        return loss, td_losses.detach()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c66ee2bd-8528-4c72-8c27-20f7aad5286d",
      "metadata": {
        "id": "c66ee2bd-8528-4c72-8c27-20f7aad5286d"
      },
      "outputs": [],
      "source": [
        "def eval_dqn(env_name, Q):\n",
        "    \"\"\"Оценка качества работы алгоритма на одном эпизоде\"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    s, _ = env.reset()\n",
        "    done, ep_return = False, 0.\n",
        "\n",
        "    while not done:\n",
        "        # set epsilon = 0 to make an agent act greedy\n",
        "        a = select_action_eps_greedy(Q, s, epsilon=0.)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "        done = terminated or truncated\n",
        "        ep_return += r\n",
        "        s = s_next\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return ep_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2be9060-bba6-4d9a-9e4b-0aa50f88839e",
      "metadata": {
        "id": "d2be9060-bba6-4d9a-9e4b-0aa50f88839e"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "def linear(st, end, duration, t):\n",
        "    \"\"\"\n",
        "    Линейная интерполяция значений в пределах диапазона [st, end],\n",
        "    используя прогресс по времени t относительно всего отведенного\n",
        "    времени duration.\n",
        "    \"\"\"\n",
        "\n",
        "    if t >= duration:\n",
        "        return end\n",
        "    return st + (end - st) * (t / duration)\n",
        "\n",
        "def run_dqn(\n",
        "        env_name=\"CartPole-v1\",\n",
        "        hidden_dims=(128, 128), lr=1e-3, gamma=0.99,\n",
        "        eps_st=.4, eps_end=.02, eps_dur=.25, total_max_steps=100_000,\n",
        "        train_schedule=1, eval_schedule=1000, smooth_ret_window=10, success_ret=200.\n",
        "):\n",
        "    env = gym.make(env_name)\n",
        "    eval_return_history = deque(maxlen=smooth_ret_window)\n",
        "\n",
        "    Q = create_network(\n",
        "        input_dim=env.observation_space.shape[0], hidden_dims=hidden_dims, output_dim=env.action_space.n\n",
        "    )\n",
        "    opt = torch.optim.Adam(Q.parameters(), lr=lr)\n",
        "\n",
        "    s, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for global_step in range(1, total_max_steps + 1):\n",
        "        epsilon = linear(eps_st, eps_end, eps_dur * total_max_steps, global_step)\n",
        "\n",
        "        a = select_action_eps_greedy(Q, s, epsilon=epsilon)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if global_step % train_schedule == 0:\n",
        "            opt.zero_grad()\n",
        "            td_target = compute_td_target(Q, [r], [s_next], [terminated], gamma=gamma)\n",
        "            loss = compute_td_loss(Q, [s], [a], td_target)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        if global_step % eval_schedule == 0:\n",
        "            eval_return = eval_dqn(env_name, Q)\n",
        "            eval_return_history.append(eval_return)\n",
        "            avg_return = np.mean(eval_return_history)\n",
        "            print(f'{global_step=} | {avg_return=:.3f} | {epsilon=:.3f}')\n",
        "            if avg_return >= success_ret:\n",
        "                print('Решено!')\n",
        "                break\n",
        "\n",
        "        s = s_next\n",
        "        if done:\n",
        "            s, _ = env.reset()\n",
        "            done = False\n",
        "\n",
        "run_dqn(eval_schedule=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6aa1f1e-f346-4081-9aa9-4a1f6655152c",
      "metadata": {
        "id": "b6aa1f1e-f346-4081-9aa9-4a1f6655152c"
      },
      "source": [
        "Комментарии к получаемым результатам:\n",
        "- `avg_return` - это средняя отдача за эпизод на истории из последних десяти эпизодов. В случае корректной реализации, этот показатель будет низким первые 1000 шагов и только затем будет возрастать и сойдется на 5000-15000 шагах в зависимости от архитектуры сети.\n",
        "- Если сеть не достигает нужных результатов к концу цикла, попробуйте увеличить число нейронов в скрытом слое или поменяйте начальный $\\epsilon$.\n",
        "- Переменная `epsilon` обеспечивает стремление агента исследовать среду. В данной реализации используется линейное затухание для частоты исследования."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be1c2a8-37c6-413d-9d57-eff7251943ab",
      "metadata": {
        "id": "6be1c2a8-37c6-413d-9d57-eff7251943ab"
      },
      "source": [
        "### DQN with Experience Replay\n",
        "\n",
        "Теперь попробуем добавить поддержку памяти прецедентов (Replay Buffer), которая будет из себя представлять очередь из наборов: $\\{(s, a, r, s', 1_\\text{terminated})\\}$.\n",
        "\n",
        "Тогда во время обучения каждый новый переход будет добавляться в память, а обучение будет целиком производиться на переходах, просэмплированных из памяти прецедентов."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1 балл)"
      ],
      "metadata": {
        "id": "HMUBrOVvOpg1"
      },
      "id": "HMUBrOVvOpg1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7b5af1a-cd63-4642-9ec2-f1dc71215dff",
      "metadata": {
        "id": "a7b5af1a-cd63-4642-9ec2-f1dc71215dff"
      },
      "outputs": [],
      "source": [
        "def sample_batch(replay_buffer, n_samples):\n",
        "    # sample randomly `n_samples` samples from replay buffer\n",
        "    # and split an array of samples into arrays:\n",
        "    #    states, actions, rewards, next_states, terminateds\n",
        "    # Use np.random.default_rng().choice for sampling\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(terminateds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a8db42-6e80-4a38-8449-f13c50ef849c",
      "metadata": {
        "id": "76a8db42-6e80-4a38-8449-f13c50ef849c"
      },
      "outputs": [],
      "source": [
        "def run_dqn_rb(\n",
        "        env_name=\"CartPole-v1\",\n",
        "        hidden_dims=(256, 256), lr=1e-3, gamma=0.99,\n",
        "        eps_st=.4, eps_end=.02, eps_dur=.25, total_max_steps=100_000,\n",
        "        train_schedule=4, replay_buffer_size=400, batch_size=32,\n",
        "        eval_schedule=1000, smooth_ret_window=5, success_ret=200.\n",
        "):\n",
        "    env = gym.make(env_name)\n",
        "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
        "    eval_return_history = deque(maxlen=smooth_ret_window)\n",
        "\n",
        "    Q = create_network(\n",
        "        input_dim=env.observation_space.shape[0], hidden_dims=hidden_dims, output_dim=env.action_space.n\n",
        "    )\n",
        "    opt = torch.optim.Adam(Q.parameters(), lr=lr)\n",
        "\n",
        "    s, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for global_step in range(1, total_max_steps + 1):\n",
        "        epsilon = linear(eps_st, eps_end, eps_dur * total_max_steps, global_step)\n",
        "        a = select_action_eps_greedy(Q, s, epsilon=epsilon)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        replay_buffer.append((s, a, r, s_next, terminated))\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if global_step % train_schedule == 0:\n",
        "            train_batch = sample_batch(replay_buffer, batch_size)\n",
        "            states, actions, rewards, next_states, terminateds = train_batch\n",
        "\n",
        "            opt.zero_grad()\n",
        "            td_target = compute_td_target(Q, rewards, next_states, terminateds, gamma=gamma)\n",
        "            loss = compute_td_loss(Q, states, actions, td_target)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        if global_step % eval_schedule == 0:\n",
        "            eval_return = eval_dqn(env_name, Q)\n",
        "            eval_return_history.append(eval_return)\n",
        "            avg_return = np.mean(eval_return_history)\n",
        "            print(f'{global_step=} | {avg_return=:.3f} | {epsilon=:.3f}')\n",
        "            if avg_return >= success_ret:\n",
        "                print('Решено!')\n",
        "                break\n",
        "\n",
        "        s = s_next\n",
        "        if done:\n",
        "            s, _ = env.reset()\n",
        "            done = False\n",
        "\n",
        "run_dqn_rb(eval_schedule=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "271c3132-f2eb-4d5a-8ba4-8811421ce3cd",
      "metadata": {
        "id": "271c3132-f2eb-4d5a-8ba4-8811421ce3cd"
      },
      "source": [
        "## DQN with Prioritized Experience Replay\n",
        "\n",
        "Добавим каждому примеру, хранящемуся в памяти, значение приоритета. Приоритет будет влиять на частоту случайного выбора примеров в пакет на обучение. Удачный выбор приоритета позволит повысить эффективность обучения. Популярным вариантом является абсолютное значение TD-ошибки. Таким образом акцент при обучении Q-функции отводится примерам, на которых аппроксиматор ошибается сильнее.\n",
        "\n",
        "Однако, нужно помнить, что это значение быстро устаревает, если его не обновлять. Но и обновлять для всей памяти каждый раз накладно. Из-за этого потребуется искать баланс между точностью оценки приоритета и скоростью работы.\n",
        "\n",
        "В данном задании мы будем делать следующее:\n",
        "\n",
        "- Использовать TD-ошибку в качестве приоритета.\n",
        "- Так как для пакета данных, используемых при обучении, в любом случае будет вычислена TD-ошибка, воспользуемся полученными значениями для обновления значений приоритета в памяти для каждого примера из данного пакета.\n",
        "- Будем периодически сортировать память для того, чтобы новые добавляемые переходы заменяли собой те переходы, у которых наименьший приоритет (т.е. наименьшие значения ошибки). Сортировка - дорогостоящая операция, поэтому выбрана редкая периодичность.\n",
        "\n",
        "NB: Обратите внимание, что софтмакс очень чувствителен к масштабу величин и часто требует подбора температуры. Чтобы частично нивелировать эту проблему, предлагается использовать не `softmax(priorities)` напрямую, а воспользоваться функцией $\\text{symlog} = \\text{sign}(x) \\cdot \\log (|x| + 1)$, то есть `softmax(symlog(priorities))`, и не подбирать температуру. Идея взята из статьи DreamerV2 —-- в этой статье можно ознакомиться с идеей применения функций *symlog* и *simexp*, так как это полезная альтернатива нормализации некоторых величин в RL (вознаграждений, отдач, полезностей, логитов)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2 балла)"
      ],
      "metadata": {
        "id": "_WoesTAWOxs0"
      },
      "id": "_WoesTAWOxs0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ae2d18b-bb97-4a8f-94ee-259935c039b9",
      "metadata": {
        "id": "3ae2d18b-bb97-4a8f-94ee-259935c039b9"
      },
      "outputs": [],
      "source": [
        "def symlog(x):\n",
        "    \"\"\"\n",
        "    Compute symlog values for a vector `x`.\n",
        "    It's an inverse operation for symexp.\n",
        "    \"\"\"\n",
        "    return np.sign(x) * np.log(np.abs(x) + 1)\n",
        "\n",
        "def softmax(xs, temp=1.):\n",
        "    exp_xs = np.exp((xs - xs.max()) / temp)\n",
        "    return exp_xs / exp_xs.sum()\n",
        "\n",
        "def sample_prioritized_batch(replay_buffer, n_samples):\n",
        "    # Sample randomly `n_samples` examples from replay buffer\n",
        "    # weighting by priority (example's TD error) and split an array\n",
        "    # of sample tuples into arrays:\n",
        "    #    states, actions, rewards, next_states, terminateds\n",
        "    # Also, keep samples' indices (into `indices`) to return them too!\n",
        "    # Note that each sample in replay buffer is a tuple:\n",
        "    #   (priority, state, action, reward, next_state, terminated)\n",
        "    # Use\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    batch = (\n",
        "        np.array(states), np.array(actions), np.array(rewards),\n",
        "        np.array(next_states), np.array(terminateds)\n",
        "    )\n",
        "    return batch, indices\n",
        "\n",
        "def update_batch(replay_buffer, indices, batch, new_priority):\n",
        "    \"\"\"Updates batches with corresponding indices\n",
        "    replacing their priority values.\"\"\"\n",
        "    states, actions, rewards, next_states, terminateds = batch\n",
        "\n",
        "    for i in range(len(indices)):\n",
        "        new_batch = (\n",
        "            new_priority[i], states[i], actions[i], rewards[i],\n",
        "            next_states[i], terminateds[i]\n",
        "        )\n",
        "        replay_buffer[indices[i]] = new_batch\n",
        "\n",
        "def sort_replay_buffer(replay_buffer):\n",
        "    \"\"\"Sorts replay buffer to move samples with\n",
        "    lesser priority to the beginning ==> they will be\n",
        "    replaced with the new samples sooner.\"\"\"\n",
        "    new_rb = deque(maxlen=replay_buffer.maxlen)\n",
        "    new_rb.extend(sorted(replay_buffer, key=lambda sample: sample[0]))\n",
        "    return new_rb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1 балл)"
      ],
      "metadata": {
        "id": "CTvjHIaJO1Oe"
      },
      "id": "CTvjHIaJO1Oe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88dcccfe-159d-48b3-adcb-d1d5459cdf84",
      "metadata": {
        "id": "88dcccfe-159d-48b3-adcb-d1d5459cdf84"
      },
      "outputs": [],
      "source": [
        "def run_dqn_prioritized_rb(\n",
        "        env_name=\"CartPole-v1\",\n",
        "        hidden_dims=(256, 256), lr=1e-3, gamma=0.99,\n",
        "        eps_st=.4, eps_end=.02, eps_dur=.25, total_max_steps=100_000,\n",
        "        train_schedule=4, replay_buffer_size=400, batch_size=32,\n",
        "        eval_schedule=1000, smooth_ret_window=5, success_ret=200.\n",
        "):\n",
        "    env = gym.make(env_name)\n",
        "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
        "    eval_return_history = deque(maxlen=smooth_ret_window)\n",
        "\n",
        "    Q = create_network(\n",
        "        input_dim=env.observation_space.shape[0], hidden_dims=hidden_dims,\n",
        "        output_dim=env.action_space.n\n",
        "    )\n",
        "    opt = torch.optim.Adam(Q.parameters(), lr=lr)\n",
        "\n",
        "    s, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for global_step in range(1, total_max_steps + 1):\n",
        "        epsilon = linear(\n",
        "            eps_st, eps_end, eps_dur * total_max_steps, global_step\n",
        "        )\n",
        "        a = select_action_eps_greedy(Q, s, epsilon=epsilon)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        # Compute new sample loss (compute w/o gradients!)\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "\n",
        "        replay_buffer.append((loss, s, a, r, s_next, terminated))\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if global_step % train_schedule == 0:\n",
        "            train_batch, indices = sample_prioritized_batch(\n",
        "                replay_buffer, batch_size\n",
        "            )\n",
        "            (\n",
        "                states, actions, rewards,\n",
        "                next_states, terminateds\n",
        "            ) = train_batch\n",
        "\n",
        "            opt.zero_grad()\n",
        "            td_target = compute_td_target(Q, rewards, next_states, terminateds, gamma=gamma)\n",
        "            loss, td_losses = compute_td_loss(Q, states, actions, td_target, out_non_reduced_losses=True)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            update_batch(\n",
        "                replay_buffer, indices, train_batch, td_losses.numpy()\n",
        "            )\n",
        "\n",
        "        # with much slower scheduler periodically re-sort replay buffer\n",
        "        # such that it will overwrite the least important samples\n",
        "        if global_step % (10 * train_schedule) == 0:\n",
        "            replay_buffer = sort_replay_buffer(replay_buffer)\n",
        "\n",
        "        if global_step % eval_schedule == 0:\n",
        "            eval_return = eval_dqn(env_name, Q)\n",
        "            eval_return_history.append(eval_return)\n",
        "            avg_return = np.mean(eval_return_history)\n",
        "            print(f'{global_step=} | {avg_return=:.3f} | {epsilon=:.3f}')\n",
        "            if avg_return >= success_ret:\n",
        "                print('Решено!')\n",
        "                break\n",
        "\n",
        "        s = s_next\n",
        "        if done:\n",
        "            s, _ = env.reset()\n",
        "            done = False\n",
        "\n",
        "run_dqn_prioritized_rb(eval_schedule=250)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}