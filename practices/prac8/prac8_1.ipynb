{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxfNqvuOw37I"
      },
      "source": [
        "## Многорукие бандиты. Контекстные бандиты.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1146/1*XU13pI_KETfwIJnc8gWS7w.png\">\n",
        "Названия метода пошло от «одноруких бандитов» — игровых автоматов в казино с рычагом, за который можно потянуть и получить выигрыш. Представьте, что вы находитесь в зале с десятком таких автоматов, и у вас есть N бесплатных попыток для игры на любом из них. Вы, конечно же, хотите выиграть побольше денег, но заранее не знаете, какой автомат дает самый большой выигрыш. Проблема многоруких бандитов как раз заключается в том, чтобы найти самый выгодный автомат с минимальными потерями"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4jGtt6t7boyw"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6_8Bm-pySc7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium.utils import seeding\n",
        "from gymnasium\n",
        "\n",
        "\n",
        "class MultiArmedBanditEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Multi-armed bandit environment implementation.\n",
        "\n",
        "    Each bandit arm's reward distribution is a composition of Bernoulli and Gaussian:\n",
        "    Bernoulli defines a probability to get \"something\" (so, 1-p prob to get\n",
        "    nothing/zero), while a Gaussian defines the value of that \"something\".\n",
        "    It also includes extreme cases: B(p=1.0), B(p=0.0), or N(mu=XXX, std=0.0)\n",
        "\n",
        "    p_dist: [float]\n",
        "        A list of Bernoulli p for each arm (=action) to pay out.\n",
        "    r_dist: [float | tuple(float, float)]\n",
        "        A list of a) rewards to pay out or b) 2-element tuples: mean and std for\n",
        "        reward Gaussian dist.\n",
        "    info:\n",
        "        Info about the environment that the agents is not supposed to know. For instance,\n",
        "        info can reveal the index of the optimal arm, or the value of prior parameter.\n",
        "        Can be useful to evaluate the agent's perfomance\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_bandits=None, p_dist=None, r_dist=None, seed=None, info=None):\n",
        "        super().__init__()\n",
        "\n",
        "        rng = np.random.default_rng(seed)\n",
        "        if n_bandits is None:\n",
        "            n_bandits = len(p_dist) if p_dist is not None else len(r_dist)\n",
        "\n",
        "        if p_dist is None:\n",
        "            p_dist = rng.random(n_bandits)\n",
        "        self.p_dist = p_dist\n",
        "\n",
        "        if r_dist is None:\n",
        "            r_dist = list(zip(rng.normal(size=n_bandits), np.ones(n_bandits)))\n",
        "        # convert to a single format: [(mu, std)]\n",
        "        self.r_dist = [\n",
        "            tuple(r) if isinstance(r, (list, tuple)) else (r, 0.0)\n",
        "            for r in r_dist\n",
        "        ]\n",
        "        self.info = {} if info is None else info\n",
        "\n",
        "        # dummy observation space: not used, only to support gym API\n",
        "        self.observation_space = spaces.Box(-1.0, 1.0, (1,))\n",
        "\n",
        "        self.n_bandits = len(p_dist)\n",
        "        self.action_space = spaces.Discrete(self.n_bandits)\n",
        "        self.best_mean_reward = max(\n",
        "            p * mu\n",
        "            for p, (mu, _) in zip(self.p_dist, self.r_dist)\n",
        "        )\n",
        "\n",
        "        if seed is not None:\n",
        "            self.reset(seed=int(rng.integers(1_000_000)))\n",
        "\n",
        "    def step(self, action):\n",
        "        rng = self.np_random\n",
        "        a = action\n",
        "        p, (mu, std) = self.p_dist[a], self.r_dist[a]\n",
        "\n",
        "        # draw from Bernoulli then draw from Gaussian\n",
        "        reward = rng.normal(mu, std) if rng.random() < p else 0.\n",
        "\n",
        "        info = self.info | {\"regret\": self.best_mean_reward - p*mu}\n",
        "        return [0], reward, True, False, info\n",
        "\n",
        "\n",
        "class TenArmedBanditGaussian(MultiArmedBanditEnv):\n",
        "    \"\"\"\n",
        "    10 armed bandit mentioned on page 30 of Sutton and Barto's\n",
        "    [Reinforcement Learning: An Introduction](https://www.dropbox.com/s/b3psxv2r0ccmf80/book2015oct.pdf?dl=0)\n",
        "    Actions always pay out\n",
        "    Mean of payout is pulled from a normal distribution (0, 1) (called q*(a))\n",
        "    Actual reward is drawn from a normal distribution (q*(a), 1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_bandits=10, seed=None):\n",
        "        p_dist = np.ones(n_bandits, dtype=float)\n",
        "        super().__init__(p_dist=p_dist, seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rzlt_8vyVbq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "env = TenArmedBanditGaussian(10, seed=None)\n",
        "\n",
        "num_steps = 10000\n",
        "regret, ret = 0, 0\n",
        "for _ in range(num_steps):\n",
        "    _, reward, _, _, info = env.step(env.action_space.sample())\n",
        "    regret += info['regret']\n",
        "    ret += reward\n",
        "\n",
        "print(regret / num_steps, ret / num_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0Dk4WGkRcKf"
      },
      "source": [
        "## Вспоминаем теорию:\n",
        "\n",
        "Оптимальная полезность $V^*$: $$V^* = Q(a^*) = \\max_{a \\in A} Q(a)$$\n",
        "\n",
        "Regret (потери) - упущенное на каждом шаге вознаграждение: $$l_t=\\mathbb{E}(V^*-Q(a_t))$$\n",
        "\n",
        "Обновление полезностей:\n",
        "$$Q_t(a) = Q_{t-1}\\frac{1}{N_t(a_t)}(r_t - Q_{t-1})$$\n",
        "\n",
        "Алгоритм Upper Confidence Bound (UCB):\n",
        "$$a_t=\\textit{arg}\\max_{a \\in A}\\left(Q(a) + \\sqrt{\\frac{c \\log{t}}{N_t(a)}}\\right)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jUp12XbzGfc"
      },
      "source": [
        "### Задание 1: Реализуйте алгоритмы: $\\epsilon-greedy$, $\\epsilon-greedy$ с затуханием, UCB и Softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2 балла)"
      ],
      "metadata": {
        "id": "jPbWLvFTcPVp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU805YM337SJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from numpy.random import Generator\n",
        "\n",
        "def softmax(xs, inv_temp=1.):\n",
        "    exp_xs = np.exp((xs - xs.max()) * inv_temp)\n",
        "    return exp_xs / exp_xs.sum()\n",
        "\n",
        "class Agent:\n",
        "    eps: float\n",
        "    eps_decay: float\n",
        "    exp_const: float\n",
        "\n",
        "    q: np.ndarray\n",
        "    n: np.ndarray\n",
        "    rng: Generator\n",
        "\n",
        "    def __init__(\n",
        "        self, name, n_actions, seed,\n",
        "        eps=0., eps_decay=1.,\n",
        "        ucb_const=0.,\n",
        "        softmax_t_inv=0., softmax_t_decay=0.\n",
        "    ):\n",
        "        self.name = name\n",
        "        # epsilon и коэфф его экспоненциального затухания\n",
        "        self.eps = eps\n",
        "        self.eps_decay = eps_decay\n",
        "        # константа в формуле UCB\n",
        "        self.ucb_const = ucb_const\n",
        "        # коэфф жесткости софтмакса T inverse и с какой скоростью он растет со временем\n",
        "        # NB: формула ниже\n",
        "        self.softmax_t_inv = softmax_t_inv\n",
        "        self.softmax_t_decay = softmax_t_decay\n",
        "\n",
        "        # q: each arm returns, n: the number of times an arm was chosen ==> q/n: mean arm value\n",
        "        # note that we init `n` with ones to remedy q/n division problems\n",
        "        self.q = np.zeros(n_actions)\n",
        "        self.n = np.ones(n_actions)\n",
        "        # total actions done (=steps)\n",
        "        self.T = n_actions\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def reinforce(self, action, reward):\n",
        "        # Update accumulated statistics\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "\n",
        "    def act(self):\n",
        "        rng = self.rng\n",
        "        q, n, T = self.q, self.n, self.T\n",
        "        n_actions = q.shape[-1]\n",
        "\n",
        "        # Implement action selection:\n",
        "        #   a) if eps > 0 then random with probability eps\n",
        "        #       eps = eps_0 * eps_decay ^ T\n",
        "        #   b) if softmax_t_inv > 0 then softmax\n",
        "        #       T_inv <- T_inv_0 * T ^ softmax_t_decay\n",
        "        #   b) else greedy action selection:\n",
        "        #       if ucb_const > 0, using UCB estimate for Q[a]\n",
        "        #       else using Q[a] estimate\n",
        "        # action =\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkbzwoPr5iDk"
      },
      "source": [
        "### Задание 2: Нарисуйте кривые потерь (regret) для каждого из алгоритмов."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2 балла)"
      ],
      "metadata": {
        "id": "10JLXUMfcU2L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VedPJq0N5jXu",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "seed = 42\n",
        "env = TenArmedBanditGaussian(seed=seed)\n",
        "n_actions = env.action_space.n\n",
        "n_trials = 200_000\n",
        "xs = np.arange(n_trials+1)\n",
        "eps_decay, ucb_const = .999, 2.\n",
        "softmax_t_inv, softmax_t_decay=1.0, 0.33\n",
        "\n",
        "# Evaluate and plot results for rand, eps-greedy, eps-greedy w/ decay and ucb agents\n",
        "agent_random = Agent('rand', n_actions, seed, eps=1.)\n",
        "agent_eps_greedy25 = Agent('eps-greedy-0.25', n_actions, seed, eps=0.25)\n",
        "agent_eps_greedy5 = Agent('eps-greedy-0.05', n_actions, seed, eps=0.05)\n",
        "agent_eps_greedy_decayed = Agent('eps-greedy-dec', n_actions, seed, eps=1.0, eps_decay=eps_decay)\n",
        "agent_ucb = Agent('ucb', n_actions, seed, ucb_const=ucb_const)\n",
        "agent_softmax = Agent('softmax', n_actions, seed, softmax_t_inv=softmax_t_inv, softmax_t_decay=softmax_t_decay)\n",
        "\n",
        "agents = [\n",
        "    # agent_random,\n",
        "    agent_eps_greedy25,\n",
        "    agent_eps_greedy5,\n",
        "    agent_eps_greedy_decayed,\n",
        "    agent_ucb,\n",
        "    agent_softmax,\n",
        "]\n",
        "\n",
        "####### Здесь ваш код ########\n",
        "raise NotImplementedError\n",
        "##############################\n",
        "\n",
        "plt.legend([agent.name for agent in agents])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dals5vML7iEJ"
      },
      "source": [
        "## [Optional] Контекстные бандиты (Online Contextual Bandits)\n",
        "\n",
        "### Описание задачи\n",
        "\n",
        "Контекстные бандиты служат для решения проблемы, аналогичной многоруким бандитам, с той лишь разницей, что на каждой итерации выбора действия есть дополнительная информация.\n",
        "\n",
        "Задача может быть описана итеративным процессом получения данных:\n",
        "\n",
        "* На каждой итерации среда генерирует наблюдение, которое состоит из вектора признаков фиксированного размера, и значение вознаграждения, который является стохастичным, но зависит от первого вектора.\n",
        "\n",
        "* Агент должен выбрать одно действие из множества действий (\"рук\") $m$.\n",
        "\n",
        "* Среда сообщает вознаграждение для руки, которую выбрал агент.\n",
        "\n",
        "Цель - построить стратегию, которая будет максимизировать вознаграждение, получаемое агентом.\n",
        "\n",
        "Примерами сценариев использования могут быть задачи онлайн рекомендаций, в которых есть лишь информация о том, нажал ли пользователь на рекламную ссылку или нет, а информации про другие ссылки, на которые он нажал - нет.\n",
        "\n",
        "Или задачи клинических испытаний, в которых мы знаем, как человек отреагировал на одно лекарство, но не знаем, как он отреагировал бы на другое.\n",
        "\n",
        "###  Алгоритмы\n",
        "\n",
        "В данном пункте мы частично рассмотрим библиотеку [Contextual Bandits](https://github.com/david-cortes/contextualbandits)\n",
        "\n",
        "Библиотека предлагает реализации большого списка алгоритмов:\n",
        "\n",
        "* Upper-confidence bounds: `BootstrappedUCB`, `LogisticUCB`, и `LinUCB`.\n",
        "\n",
        "* Thompson sampling: `BootstrappedTS`, `LogisticTS`, `LinTS`, and `SoftmaxExplorer`.\n",
        "\n",
        "* Greedy exploration: `EpsilonGreedy`\n",
        "\n",
        "* Adaptive exploration: `AdaptiveGreedy`\n",
        "\n",
        "Более полную информацию вы можете найти в [документации](https://contextual-bandits.readthedocs.io/en/latest/).\n",
        "Также, обратите внимание на [примеры](https://github.com/david-cortes/contextualbandits/tree/master/example).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIjKevj38MqA"
      },
      "source": [
        "### Устанавливаем библиотеку и скачиваем данные\n",
        "\n",
        "В данном случае используется датасет [Bibtex dataset](http://mlkd.csd.auth.gr/publication_details.asp?publicationID=278)  (см. _\"Multilabel text classification for automated tag suggestion\"_), содержащий теги, которые люди присвоили разным документам (цель - научиться предлагать теги на основе данных из этих документов). Скачать датасет можно на сайте: [Extreme Classification Repository](http://manikvarma.org/downloads/XC/XMLRepository.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmIePP4i8MTg"
      },
      "outputs": [],
      "source": [
        "!pip -q install contextualbandits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpYrpp268ukw"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=0B3lPMIHmG6vGcy1xM2pJZ09MMGM' -O bib.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV6MGHbC9fKp"
      },
      "outputs": [],
      "source": [
        "!unzip bib.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrZyv0Ro8LkU"
      },
      "source": [
        "### Считываем данные\n",
        "\n",
        "Данные представляют собой текстовый файл, используемый в libsvm и других системах, но с первой строкой, указывающей количество строк, столбцов и классов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zV9gReY7xAc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, numpy as np, re\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "\n",
        "def parse_data(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        infoline = f.readline()\n",
        "        infoline = re.sub(r\"^b'\", \"\", str(infoline))\n",
        "        n_features = int(re.sub(r\"^\\d+\\s(\\d+)\\s\\d+.*$\", r\"\\1\", infoline))\n",
        "        features, labels = load_svmlight_file(f, n_features=n_features, multilabel=True)\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    labels = mlb.fit_transform(labels)\n",
        "    features = np.array(features.todense())\n",
        "    features = np.ascontiguousarray(features)\n",
        "    return features, labels\n",
        "\n",
        "X, y = parse_data(\"Bibtex/Bibtex_data.txt\")\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LhiAWQX7zPK"
      },
      "source": [
        "## Пакетные модели\n",
        "\n",
        "Логистическая регрессия и контекстный бандит:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx_s3hVR72Y9"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from contextualbandits.online import BootstrappedUCB, BootstrappedTS, LogisticUCB, \\\n",
        "            SeparateClassifiers, EpsilonGreedy, AdaptiveGreedy, ExploreFirst, \\\n",
        "            ActiveExplorer, SoftmaxExplorer\n",
        "from copy import deepcopy\n",
        "\n",
        "nchoices = y.shape[1]\n",
        "base_algorithm = LogisticRegression(solver='lbfgs', warm_start=True)\n",
        "beta_prior = ((3./nchoices, 4), 2) # until there are at least 2 observations of each class, will use this prior\n",
        "beta_prior_ucb = ((5./nchoices, 4), 2) # UCB gives higher numbers, thus the higher positive prior\n",
        "beta_prior_ts = ((2./np.log2(nchoices), 4), 2)\n",
        "\n",
        "\n",
        "## The base algorithm is embedded in different metaheuristics\n",
        "bootstrapped_ucb = BootstrappedUCB(deepcopy(base_algorithm), nchoices = nchoices,\n",
        "                                   beta_prior = beta_prior_ucb, percentile = 80,\n",
        "                                   random_state = 1111)\n",
        "\n",
        "\n",
        "models = [bootstrapped_ucb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ4Txkd879Q_"
      },
      "outputs": [],
      "source": [
        "# These lists will keep track of the rewards obtained by each policy\n",
        "rewards_ucb, *_ = [list() for i in range(len(models))]\n",
        "\n",
        "lst_rewards = [rewards_ucb, ]\n",
        "\n",
        "# batch size - algorithms will be refit after N rounds\n",
        "batch_size = 50\n",
        "\n",
        "# initial seed - all policies start with the same small random selection of actions/rewards\n",
        "first_batch = X[:batch_size, :]\n",
        "np.random.seed(1)\n",
        "action_chosen = np.random.randint(nchoices, size=batch_size)\n",
        "rewards_received = y[np.arange(batch_size), action_chosen]\n",
        "\n",
        "# fitting models for the first time\n",
        "for model in models:\n",
        "    model.fit(X=first_batch, a=action_chosen, r=rewards_received)\n",
        "\n",
        "# these lists will keep track of which actions does each policy choose\n",
        "lst_a_ucb, *_ = [action_chosen.copy() for i in range(len(models))]\n",
        "\n",
        "lst_actions = [lst_a_ucb, ]\n",
        "\n",
        "# rounds are simulated from the full dataset\n",
        "def simulate_rounds(model, rewards, actions_hist, X_global, y_global, batch_st, batch_end):\n",
        "    np.random.seed(batch_st)\n",
        "\n",
        "    ## choosing actions for this batch\n",
        "    actions_this_batch = model.predict(X_global[batch_st:batch_end, :]).astype('uint8')\n",
        "\n",
        "    # keeping track of the sum of rewards received\n",
        "    rewards.append(y_global[np.arange(batch_st, batch_end), actions_this_batch].sum())\n",
        "\n",
        "    # adding this batch to the history of selected actions\n",
        "    new_actions_hist = np.append(actions_hist, actions_this_batch)\n",
        "\n",
        "    # now refitting the algorithms after observing these new rewards\n",
        "    np.random.seed(batch_st)\n",
        "    model.fit(X_global[:batch_end, :], new_actions_hist, y_global[np.arange(batch_end), new_actions_hist])\n",
        "\n",
        "    return new_actions_hist\n",
        "\n",
        "# now running all the simulation\n",
        "for i in range(int(np.floor(X.shape[0] / batch_size))):\n",
        "    batch_st = (i + 1) * batch_size\n",
        "    batch_end = (i + 2) * batch_size\n",
        "    batch_end = np.min([batch_end, X.shape[0]])\n",
        "\n",
        "    for model in range(len(models)):\n",
        "        lst_actions[model] = simulate_rounds(models[model],\n",
        "                                             lst_rewards[model],\n",
        "                                             lst_actions[model],\n",
        "                                             X, y,\n",
        "                                             batch_st, batch_end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17yIT6k28Dj9"
      },
      "source": [
        "### Визуализируем результаты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeyCQXiJ8A4h"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "%matplotlib inline\n",
        "\n",
        "def get_mean_reward(reward_lst, batch_size=batch_size):\n",
        "    mean_rew=list()\n",
        "    for r in range(len(reward_lst)):\n",
        "        mean_rew.append(sum(reward_lst[:r+1]) * 1.0 / ((r+1)*batch_size))\n",
        "    return mean_rew\n",
        "\n",
        "rcParams['figure.figsize'] = 16, 8\n",
        "lwd = 5\n",
        "cmap = plt.get_cmap('tab20')\n",
        "colors=plt.cm.tab20(np.linspace(0, 1, 20))\n",
        "\n",
        "ax = plt.subplot(111)\n",
        "plt.plot(get_mean_reward(rewards_ucb), label=\"Bootstrapped Upper Confidence Bound (C.I.=80%)\",linewidth=lwd,color=colors[0])\n",
        "\n",
        "plt.plot(np.repeat(y.mean(axis=0).max(),len(rewards_ucb)), label=\"Overall Best Arm (no context)\",linewidth=lwd,color=colors[1],ls='dashed')\n",
        "\n",
        "\n",
        "# import warnings\n",
        "box = ax.get_position()\n",
        "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
        "                 box.width, box.height * 1.25])\n",
        "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
        "          fancybox=True, ncol=3, prop={'size':20})\n",
        "\n",
        "\n",
        "plt.tick_params(axis='both', which='major', labelsize=25)\n",
        "plt.xticks([i*20 for i in range(8)], [i*1000 for i in range(8)])\n",
        "\n",
        "\n",
        "plt.xlabel('Rounds (models were updated every 50 rounds)', size=30)\n",
        "plt.ylabel('Cumulative Mean Reward', size=30)\n",
        "plt.title('Comparison of Online Contextual Bandit Policies\\n(Base Algorithm is Logistic Regression)\\n\\nBibtext Dataset\\n(159 categories, 1836 attributes)',size=30)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH7SHs9cboy7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}