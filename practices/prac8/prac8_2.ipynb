{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5e29da58",
      "metadata": {
        "id": "5e29da58"
      },
      "source": [
        "## Стратегии исследования в Q-обучении\n",
        "\n",
        "В этой тетрадке воспользуемся ранее реализованным алгоритмом Q-обучения и адаптируем алгоритмы исследования из тетрадки по бандитам, чтобы посмотреть, насколько наблюдаемые результаты переносятся в MDP-постановку.\n",
        "\n",
        "Для обучения будем использовать среды [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/) и [Boulder](https://sites.google.com/view/edu-gym/environments/bouldering?authuser=0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e2ad6ff-2b99-457b-9b1c-68d631b30acd",
      "metadata": {
        "id": "8e2ad6ff-2b99-457b-9b1c-68d631b30acd"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49673813",
      "metadata": {
        "id": "49673813"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from numpy.random import Generator\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14fc4fd5",
      "metadata": {
        "id": "14fc4fd5"
      },
      "outputs": [],
      "source": [
        "def show_progress(rewards_batch, log):\n",
        "    \"\"\"Функция отображения прогресса обучения.\"\"\"\n",
        "    mean_reward = np.mean(rewards_batch)\n",
        "    log.append(mean_reward)\n",
        "\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=[12, 4])\n",
        "    plt.plot(log, label='Avg returns')\n",
        "    plt.legend(loc=4)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d29e3c5-08ae-4124-be20-a1bee550a636",
      "metadata": {
        "id": "1d29e3c5-08ae-4124-be20-a1bee550a636"
      },
      "source": [
        "Ниже адаптируйте заготовку, взятую из тетради с бандитами, под алгоритм Q-обучения. Как и в том задании, реализация агента будет поддерживать одновременно все возможныые стратегии исследования. Выбор происходит по тому, какие из гиперпараметров активны:\n",
        "- если `random() < eps * decay`, действие выбирается случайно (для случайной, eps-жадной и eps-жадной с затуханием), иначе\n",
        "- если `softmax_t_inv > 0`, выбор производится по softmax с `t_inv = softmax_t_inv * T_decay ^ softmax_t_decay` обратной температурой (softmax стратегия)\n",
        "- иначе выбираем жадно на основе значений `Q[s, a] + u[a]`, где `u`:\n",
        "  - вычисляется по UCB, если `ucb_const > 0` (UCB стратегия),\n",
        "  - иначе берется `u = 0` (жадная, вторая часть eps-жадной и eps-жадной с затуханием)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2 балла)"
      ],
      "metadata": {
        "id": "cMoGZQDBcuyl"
      },
      "id": "cMoGZQDBcuyl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "159c2559",
      "metadata": {
        "id": "159c2559"
      },
      "outputs": [],
      "source": [
        "def softmax(xs, inv_temp=1.):\n",
        "    exp_xs = np.exp((xs - xs.max()) * inv_temp)\n",
        "    return exp_xs / exp_xs.sum()\n",
        "\n",
        "class Agent:\n",
        "    # Реализует агента с Q-обучением и разными вариантами\n",
        "    # исследовательской стратегии: eps-greedy, eps-greedy with decay, ucb1, softmax\n",
        "    eps: float\n",
        "    eps_decay: float\n",
        "    exp_const: float\n",
        "\n",
        "    # Q-функция\n",
        "    Q: npt.NDArray\n",
        "    # счетчик посещений пар (состояние, действие)\n",
        "    n: npt.NDArray\n",
        "    T: float\n",
        "    rng: Generator\n",
        "\n",
        "    def __init__(\n",
        "        self, name, n_states, n_actions, *, seed,\n",
        "        lr, gamma,\n",
        "        eps=0., eps_decay=1.,\n",
        "        ucb_const=0.,\n",
        "        softmax_t_inv=0., softmax_t_decay=0.,\n",
        "    ):\n",
        "        self.name = name\n",
        "        self.eps = eps\n",
        "        self.eps_decay = eps_decay\n",
        "        self.ucb_const = ucb_const\n",
        "        self.softmax_t_inv = softmax_t_inv\n",
        "        self.softmax_t_decay = softmax_t_decay\n",
        "\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.Q = np.zeros((n_states, n_actions))\n",
        "        self.n = np.ones_like(self.Q)\n",
        "        self.T = np.prod(self.n.shape)\n",
        "        self.T_decay = 0\n",
        "\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def update(self, s, a, r, next_s, done):\n",
        "        lr, gamma = self.lr, self.gamma\n",
        "        # посчитай TD ошибку и обновите Q-функцию\n",
        "        # td_error =\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "\n",
        "    def _update_visitations(self, s, a):\n",
        "        # Update visitations\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "\n",
        "    def act(self, state):\n",
        "        rng = self.rng\n",
        "        s, Q, n, T = state, self.Q, self.n, self.T\n",
        "        n_actions = Q.shape[-1]\n",
        "\n",
        "        # Implement action selection:\n",
        "        #   a) if eps > 0 then random with probability eps\n",
        "        #   b) if softmax_t_inv > 0 then softmax\n",
        "        #   b) else greedy action selection:\n",
        "        #       if ucb_const > 0, using UCB estimate for Q[s,a]\n",
        "        #       else using Q[s,a] estimate\n",
        "        # action =\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "\n",
        "        self._update_visitations(s, action)\n",
        "        return action\n",
        "\n",
        "    def decay_eps(self):\n",
        "        self.T_decay += 1\n",
        "\n",
        "    @property\n",
        "    def exploration_param(self):\n",
        "        # for debug purposes return current exploration param:\n",
        "        # Для eps-жадной: eps * decay, Softmax: t_inv, UCB: u (на основе self.n.mean())\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "        return 0."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1 балл)"
      ],
      "metadata": {
        "id": "hP0qmgdgdKMm"
      },
      "id": "hP0qmgdgdKMm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f6bd68-cf91-4fbf-8d2b-0139fdb8732a",
      "metadata": {
        "id": "57f6bd68-cf91-4fbf-8d2b-0139fdb8732a"
      },
      "outputs": [],
      "source": [
        "seed = 41\n",
        "env = gym.make('Taxi-v3', max_episode_steps=200)\n",
        "\n",
        "n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "\n",
        "# гиперпараметры алгоритма\n",
        "lr = 0.02\n",
        "gamma = 0.97\n",
        "eps_decay, ucb_const = .998, 2.\n",
        "softmax_t_inv, softmax_t_decay=0.2, 0.6\n",
        "n_steps, schedule = 500_000, 5_000\n",
        "\n",
        "agent_random = Agent('rand', n_states, n_actions, seed=seed, lr=lr, gamma=gamma, eps=1.)\n",
        "agent_eps_greedy25 = Agent('eps-greedy_0.25', n_states, n_actions, seed=seed, lr=lr, gamma=gamma, eps=0.25)\n",
        "agent_eps_greedy5 = Agent('eps-greedy_0.05', n_states, n_actions, seed=seed, lr=lr, gamma=gamma, eps=0.05)\n",
        "agent_eps_greedy_decayed = Agent('eps-greedy-dec', n_states, n_actions, seed=seed, lr=lr, gamma=gamma, eps=1.0, eps_decay=eps_decay)\n",
        "agent_ucb = Agent('ucb', n_states, n_actions, seed=seed, lr=lr, gamma=gamma, ucb_const=ucb_const)\n",
        "agent_softmax = Agent('softmax', n_states, n_actions, seed=seed, lr=lr, gamma=gamma, softmax_t_inv=softmax_t_inv, softmax_t_decay=softmax_t_decay)\n",
        "\n",
        "agents = [\n",
        "    # agent_random,\n",
        "    agent_eps_greedy25,\n",
        "    agent_eps_greedy5,\n",
        "    agent_eps_greedy_decayed,\n",
        "    agent_ucb,\n",
        "    agent_softmax,\n",
        "]\n",
        "\n",
        "log = []\n",
        "for agent in agents:\n",
        "    log.append([])\n",
        "    agent_returns = None\n",
        "    step, ep = 0, 1\n",
        "    while step < n_steps:\n",
        "        s, _ = env.reset()\n",
        "        ep_ret = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            # реализуйте цикл взаимодействия со средой в рамках эпизода\n",
        "            ####### Здесь ваш код ########\n",
        "            raise NotImplementedError\n",
        "            ##############################\n",
        "\n",
        "            if step % schedule == 0:\n",
        "                show_progress(agent_returns, log[-1])\n",
        "                print(f\"[{agent.name}] Step: {step}, Episode: {ep}, Return: {agent_returns:.2f}, Exploration: {agent.exploration_param}\")\n",
        "\n",
        "        ep += 1\n",
        "        agent.decay_eps()\n",
        "        if agent_returns is None:\n",
        "            agent_returns = ep_ret\n",
        "        # делаем экспоненциальное сглаживание графика\n",
        "        agent_returns += 0.02 * (ep_ret - agent_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f6a4669-316b-487c-b2a4-9333257aa96e",
      "metadata": {
        "id": "6f6a4669-316b-487c-b2a4-9333257aa96e"
      },
      "outputs": [],
      "source": [
        "# Compare results\n",
        "plt.figure(figsize=[12, 6])\n",
        "for agent_log in log:\n",
        "    _ = plt.plot(agent_log)\n",
        "plt.legend([agent.name for agent in agents])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1 балл)"
      ],
      "metadata": {
        "id": "xD5qrA5Zd_gK"
      },
      "id": "xD5qrA5Zd_gK"
    },
    {
      "cell_type": "markdown",
      "id": "cb7c07cc-7718-4899-be68-d81db4722d0b",
      "metadata": {
        "id": "cb7c07cc-7718-4899-be68-d81db4722d0b"
      },
      "source": [
        "**Вопросы**: Что можно сказать о динамике исследования агентом? Какие из исследовательских стратегий более универсальны с точки зрения подбора гиперпараметров под среду?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9b43711-3cfc-442d-89e0-21a37ea4a996",
      "metadata": {
        "id": "c9b43711-3cfc-442d-89e0-21a37ea4a996"
      },
      "source": [
        "### Bouldering\n",
        "\n",
        "Давайте попробуем проверить реализованные алгоритмы в среде Boulder, которая требует от агента выполнить определенную последовательность действий (при выборе неверного действия в состоянии, он \"падает\" в начало, эпизод при этом продолжается). Сложность среды определяется высотой (=число состояний) и шириной трассы (=число действий).\n",
        "\n",
        "Для отладки, имеет смысл взять сначала небольшие значения (например, h=3, w=2) и постепенно увеличивать их."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7e3bdf5-ea53-490a-a7a4-e99ee4eb1e03",
      "metadata": {
        "id": "e7e3bdf5-ea53-490a-a7a4-e99ee4eb1e03"
      },
      "outputs": [],
      "source": [
        "from gymnasium import spaces\n",
        "\n",
        "class BoulderEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"terminal\", \"human\"]}\n",
        "    def __init__(self, render_mode=None, height=10, n_grips=2, max_steps=1000, success_prob=0.0):\n",
        "        '''\n",
        "        |- |\n",
        "        |- |   '-': grip\n",
        "        | -|   '*': agent\n",
        "        | -|\n",
        "        _*__\n",
        "        '''\n",
        "        self.height = height\n",
        "        self.n_grips = n_grips\n",
        "        self.max_steps = max_steps\n",
        "        self.steps_taken = 0\n",
        "        self.success_prob = success_prob\n",
        "\n",
        "        # Observation is the current height of the agent.\n",
        "        self.observation_space = spaces.Discrete(self.height+1)\n",
        "\n",
        "        # We have n_grips actions, every time the agent needs to grip the right grips\n",
        "        self.action_space = spaces.Discrete(self.n_grips)\n",
        "\n",
        "        self.pygame_initialized = False\n",
        "\n",
        "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def _get_initial_loc(self):\n",
        "        return 0\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return self._agent_location\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # We need the following line to seed self.np_random\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Initialize positions of grips\n",
        "        self._agent_location = self._get_initial_loc()\n",
        "        self.grips = self.np_random.choice(self.n_grips, self.height)\n",
        "        self.steps_taken = 0\n",
        "\n",
        "        observation = self._get_obs()\n",
        "\n",
        "        return observation, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # if the action match the given grip\n",
        "        if action == self.grips[self._agent_location]:\n",
        "            self._agent_location += 1\n",
        "        elif self.np_random.random() < self.success_prob:\n",
        "            # got lucky\n",
        "            self._agent_location += 1\n",
        "        else:\n",
        "            self._agent_location = self._get_initial_loc()\n",
        "\n",
        "        if self._agent_location == self.height:\n",
        "            # print(\"REACHED THE TARGET\")\n",
        "            reward = 1\n",
        "            terminated = True\n",
        "            truncated = False\n",
        "        else:\n",
        "            reward = 0\n",
        "            terminated = False\n",
        "            truncated = False\n",
        "\n",
        "        self.steps_taken += 1\n",
        "\n",
        "        if self.steps_taken == self.max_steps:\n",
        "            # print(\"MAX STEPS IS REACHED\")\n",
        "            terminated = False\n",
        "            truncated = True\n",
        "\n",
        "        observation = self._get_obs()\n",
        "\n",
        "        return observation, reward, terminated, truncated, {}\n",
        "\n",
        "gym.register(\n",
        "    id=f\"Boulder-v0\",\n",
        "    entry_point=\"__main__:BoulderEnv\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf74d77",
      "metadata": {
        "id": "fdf74d77"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Boulder-v0\")\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1 балл)"
      ],
      "metadata": {
        "id": "nvm-npp3dRBg"
      },
      "id": "nvm-npp3dRBg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e966ddc8-5ba7-45b5-91ab-bbc48001e025",
      "metadata": {
        "id": "e966ddc8-5ba7-45b5-91ab-bbc48001e025"
      },
      "outputs": [],
      "source": [
        "seed = 41\n",
        "env = gym.make('Boulder-v0', height=7, n_grips=4, success_prob=0.05, max_episode_steps=200)\n",
        "\n",
        "n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "\n",
        "# гиперпараметры алгоритма\n",
        "lr = 0.02\n",
        "gamma = 0.98\n",
        "eps_decay, ucb_const = .9992, 2.\n",
        "softmax_t_inv, softmax_t_decay=0.2, 0.6\n",
        "n_steps, schedule = 500_000, 10_000\n",
        "\n",
        "agent_random = Agent('rand', n_states, n_actions, seed=seed, lr=lr, gamma=gamma, eps=1.)\n",
        "agent_eps_greedy25 = Agent('eps-greedy_0.25', n_states, n_actions, seed=seed, lr=lr, gamma=gamma, eps=0.25)\n",
        "agent_eps_greedy5 = Agent('eps-greedy_0.05', n_states, n_actions, seed=seed, lr=lr, gamma=gamma, eps=0.05)\n",
        "agent_eps_greedy_decayed = Agent('eps-greedy-dec', n_states, n_actions, seed=seed, lr=lr, gamma=gamma, eps=1.0, eps_decay=eps_decay)\n",
        "agent_ucb = Agent('ucb', n_states, n_actions, seed=seed, lr=lr, gamma=gamma, ucb_const=ucb_const)\n",
        "agent_softmax = Agent('softmax', n_states, n_actions, seed=seed, lr=lr, gamma=gamma, softmax_t_inv=softmax_t_inv, softmax_t_decay=softmax_t_decay)\n",
        "\n",
        "agents = [\n",
        "    # agent_random,\n",
        "    agent_eps_greedy25,\n",
        "    agent_eps_greedy5,\n",
        "    agent_eps_greedy_decayed,\n",
        "    agent_ucb,\n",
        "    agent_softmax,\n",
        "]\n",
        "\n",
        "log = []\n",
        "for agent in agents:\n",
        "    log.append([])\n",
        "    agent_returns = None\n",
        "    step, ep = 0, 1\n",
        "    while step < n_steps:\n",
        "        s, _ = env.reset(seed=seed)\n",
        "        ep_ret = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            # реализуйте цикл взаимодействия со средой в рамках эпизода\n",
        "            ####### Здесь ваш код ########\n",
        "            raise NotImplementedError\n",
        "            ##############################\n",
        "\n",
        "            if step % schedule == 0:\n",
        "                show_progress(agent_returns, log[-1])\n",
        "                print(f\"[{agent.name}] Step: {step}, Episode: {ep}, Return: {agent_returns:.2f}, Exploration: {agent.exploration_param}\")\n",
        "\n",
        "        ep += 1\n",
        "        agent.decay_eps()\n",
        "        if agent_returns is None:\n",
        "            agent_returns = ep_ret\n",
        "        agent_returns += 0.02 * (ep_ret - agent_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5315370-73e4-4c2c-8f15-c6579de3d158",
      "metadata": {
        "id": "c5315370-73e4-4c2c-8f15-c6579de3d158"
      },
      "outputs": [],
      "source": [
        "# Compare results\n",
        "plt.figure(figsize=[12, 6])\n",
        "for agent_log in log:\n",
        "    _ = plt.plot(agent_log)\n",
        "plt.legend([agent.name for agent in agents])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1 балл)"
      ],
      "metadata": {
        "id": "BPdl50KQeGjn"
      },
      "id": "BPdl50KQeGjn"
    },
    {
      "cell_type": "markdown",
      "id": "b4fedd0e-fb7f-4a3b-853b-534fab8f193f",
      "metadata": {
        "id": "b4fedd0e-fb7f-4a3b-853b-534fab8f193f"
      },
      "source": [
        "**Вопросы**: Почему стратегии с константной eps-жадностью сильно деградируют с ростом сложности задачи?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75237165",
      "metadata": {
        "id": "75237165"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}