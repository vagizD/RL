{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhz299hpqUdE"
      },
      "source": [
        "# Предобучение навыков (options)\n",
        "\n",
        "Нашей задачей будет создание набора навыков по достижению определенных состояний в задаче такси. Для обучения навыков и стратегии над ними воспользуемся полученной ранее реализацией алгоритма Q-обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-G87DrbqUdH"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzxD-PzVqUdJ"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "env = environment = gym.make('Taxi-v3', render_mode='ansi')\n",
        "env.reset()\n",
        "# розовый текст: пассажир; синий текст: его дом; желтый: агент (такси)\n",
        "print(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBGAkCIiqUdK"
      },
      "source": [
        "Для начала добавим стандартную реализацию Q-обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyHW9s8yqUdK"
      },
      "outputs": [],
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, learning_rate, epsilon, gamma, get_legal_actions, seed=1337):\n",
        "        self.get_legal_actions = get_legal_actions\n",
        "        # when called, non-existent values appear as zeros\n",
        "        self.Q = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = gamma\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def Qs(self, state, actions):\n",
        "        return np.array([self.Q[state][action] for action in actions])\n",
        "\n",
        "    def V(self, state):\n",
        "        \"\"\"\n",
        "        Вычислить полезность состояния относительно целевой (жадной)\n",
        "        стратегии над доступными (legal) действиями.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "        if not possible_actions:\n",
        "            return 0.\n",
        "\n",
        "        return np.max(self.Qs(state, possible_actions))\n",
        "\n",
        "\n",
        "    def act(self, state, greedy=False):\n",
        "        \"\"\"Выбрать эпсилон-жадное действие.\"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "        if not possible_actions:\n",
        "            return None\n",
        "\n",
        "        if not greedy and self.rng.random() < self.epsilon:\n",
        "            action = self.rng.choice(possible_actions)\n",
        "        else:\n",
        "            best_action_ind = np.argmax(self.Qs(state, possible_actions))\n",
        "            action = possible_actions[best_action_ind]\n",
        "        return action\n",
        "\n",
        "    def update(self, state, action, next_state, reward, terminated):\n",
        "        \"\"\"Сделать шаг Q-обучения.\"\"\"\n",
        "        # переименуем для краткости формул\n",
        "        lr, gamma = self.learning_rate, self.gamma\n",
        "        r, s, a, next_s = reward, state, action, next_state\n",
        "        not_terminated = int(not terminated)\n",
        "        Q, V = self.Q, self.V\n",
        "\n",
        "        td_error = r + gamma * not_terminated * V(next_s) - Q[s][a]\n",
        "        Q[s][a] += lr * td_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfTji9-ZqUdK"
      },
      "source": [
        "### Подготовка среды\n",
        "\n",
        "Теперь разберемся как реализована среда Taxi: https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/toy_text/taxi.py\n",
        "\n",
        "![taxi](https://gymnasium.farama.org/_images/taxi.gif)\n",
        "\n",
        "В данной клеточной среде агент выступает в качестве таксиста, которому требуется забрать пассажира и довезти его до дома. Каждый эпизод позиции пассажира и дома выбираются случайно из четырех возможных, отмеченных цветом. Начальная позиция агента также выбирается случайно и может быть любой незанятой.\n",
        "\n",
        "Так как возможные позиции пассажира и дома ограничены четырьмя клетками, агенту достаточно выучить всего четыре соответствующих им навыка, перемещающих его из любой точки в целевую. Тогда решением задачи будет последовательная комбинация двух навыков \"добраться до клетки с пассажиром\", \"подобрать его\", \"добраться до клетки с домом\".\n",
        "\n",
        "Чтобы обучить такие навыки, создадим 4 модифицированных окружения, аналогичных Taxi, в которых целью агента будет достижение только одной из возможных точек R, G, B, Y."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1 балл)"
      ],
      "metadata": {
        "id": "4yf4n-Z3sb0z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JSB2AaaqUdL"
      },
      "outputs": [],
      "source": [
        "class FixedTargetTaxi(gym.Wrapper):\n",
        "    def __init__(self, env, target_id, target_reward):\n",
        "        super().__init__(env)\n",
        "        # locs хранит список кортежей вида (row, col) всех\n",
        "        # четырех целевых точек\n",
        "        self._target = self.unwrapped.locs[target_id]\n",
        "        self._target_reward = target_reward\n",
        "\n",
        "    def step(self, action):\n",
        "        # получаем выход метода step среды,\n",
        "        # игнорируем вознаграждение и флаг terminated от среды,\n",
        "        # задаем его сами на основе проверки достижения цели,\n",
        "        # за каждое действие будем давать вознаграждение\n",
        "        # -1, за достижение цели - self._target_reward\n",
        "        state, _, _, truncated, obs = super().step(action)\n",
        "\n",
        "        # получаем координаты такси\n",
        "        row, col, *_ = self.unwrapped.decode(state)\n",
        "        taxi_pos = (row, col)\n",
        "\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "        return state, reward, terminated, truncated, obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p8zA5SnqUdL"
      },
      "source": [
        "Проверим нашу обертку среды (wrapper), используя случайную стратегию. Порядок точек должен быть следующим: R, G, Y, B."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2 балла)"
      ],
      "metadata": {
        "id": "_XNaaEZ0sh1o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc-DAZDjqUdM"
      },
      "outputs": [],
      "source": [
        "# для каждой из 4х возможных целей\n",
        "for target in range(4):\n",
        "    # создаем окружение с заданной целью\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    # применяем случайную стратегию,\n",
        "    # пока эпизод не завершится\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    # на отрисовке нужно смотреть на позицию клетки желтого цвета\n",
        "    print(env.render())\n",
        "    print(f\"state:{state} reward:{reward} ret:{ret}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz9rG51oqUdM"
      },
      "source": [
        "Теперь добавим функцию с циклом обучения агента:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnCB4YjyqUdN"
      },
      "outputs": [],
      "source": [
        "def train_episode(env, agent, is_eval=False):\n",
        "    s, _ = env.reset()\n",
        "    ret = 0.0\n",
        "\n",
        "    while True:\n",
        "        a = agent.act(s, greedy=is_eval)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "        if not is_eval:\n",
        "            agent.update(s, a, next_s, r, terminated)\n",
        "        s = next_s\n",
        "        ret += r\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaO-4XTJqUdN"
      },
      "source": [
        "### Обучение навыков\n",
        "\n",
        "Теперь для каждой из четырех возможных целей обучим отдельную стратегию, которую в будущем сможем использовать в качестве навыка. Для обучения навыков создадим четыре пары \"агент\"-\"среда с фиксированной целью\" и обучим каждого агента на соответствующей среде."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2 балла)"
      ],
      "metadata": {
        "id": "haeX_omUsluA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3dluki2qUdN"
      },
      "outputs": [],
      "source": [
        "rng = np.random.default_rng()\n",
        "\n",
        "# общие параметры инициализации агентов\n",
        "params = dict(\n",
        "    learning_rate=0.1,\n",
        "    epsilon=0.1,\n",
        "    gamma=0.99,\n",
        "    get_legal_actions=lambda s: range(4),\n",
        ")\n",
        "\n",
        "# создаем агентов\n",
        "agents_for_options = [\n",
        "    QLearningAgent(**params, seed=rng.integers(1_000_000)) for _ in range(4)\n",
        "]\n",
        "\n",
        "for index in range(4):\n",
        "    # создаем обертку окружения с фиксированной целью,\n",
        "    # используя созданных окружения обучаем агентов\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jSpW99fqUdN"
      },
      "source": [
        "Для того, чтобы превратить каждую из обученных стратегий четырех агентов в навык (option), требуется дополнительно определить область её действия. Область действия задается множеством начальных состояний, из которых навык может быть начат, и терминальными вероятностями, то есть вероятностями завершения навыка в каждом состоянии."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNtY4gO_qUdN"
      },
      "outputs": [],
      "source": [
        "class Option:\n",
        "    def __init__(self, policy, initial_states, termination_prob, seed=None):\n",
        "        self.policy = policy\n",
        "        self.termination_prob = termination_prob\n",
        "        self.initial_states = initial_states\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def can_start(self, state):\n",
        "        return state in self.initial_states\n",
        "\n",
        "    def is_terminated(self, state):\n",
        "        \"\"\"Проверяет необходимость завершения выполнения навыка\"\"\"\n",
        "        return self.rng.random() <= self.termination_prob[state]\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Выбирает действие в соответствии со стратегией навыка\"\"\"\n",
        "        # обратите внимание, что навык исполняется с жадной стратегией,\n",
        "        # т.к. нам не нужно исследование для уже обученного навыка\n",
        "        return self.policy.act(state, greedy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQDCHIZKqUdO"
      },
      "source": [
        "Создадим объекты навыков на базе обученных ранее стратегий"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1 балл)"
      ],
      "metadata": {
        "id": "4txIQDFpspGD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osI7nLAWqUdO"
      },
      "outputs": [],
      "source": [
        "options = []\n",
        "for index, agent in enumerate(agents_for_options):\n",
        "    # словарь вида состояние: вероятность завершения навыка\n",
        "    # (используйте бинарные вероятности 0.0 и 1.0)\n",
        "    termination_prob = {}\n",
        "    # состояния, в которых навык можно вызвать\n",
        "    # (все кроме целевых)\n",
        "    initial_states = set()\n",
        "\n",
        "    # используйте env.decode(state) для определения координат по\n",
        "    # состоянию и env.locs[index] для определения координат цели.\n",
        "    for state in range(500):\n",
        "        ####### Здесь ваш код ########\n",
        "        raise NotImplementedError\n",
        "        ##############################\n",
        "\n",
        "    options.append(\n",
        "        Option(\n",
        "            policy=agent,\n",
        "            termination_prob=termination_prob,\n",
        "            initial_states=initial_states\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "86vs2O4kqUdO"
      },
      "source": [
        "Далее напишем функцию, которая исполняет навык в среде и возвращает совокупное вознаграждение, то есть дисконтированную сумму вознаграждений от среды за все время исполнения навыка:\n",
        "\n",
        "$$R = r_{1} + \\gamma r_{2} + \\gamma^{2} r_{3} + \\dots + \\gamma^{t-1}r_{t}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2 балла)"
      ],
      "metadata": {
        "id": "km6-Oi6Vsra3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU8mYSWJqUdO"
      },
      "outputs": [],
      "source": [
        "def apply_option(option, gamma, env, debug=False):\n",
        "    reward = 0\n",
        "    gamma_t = 1.0\n",
        "\n",
        "    state = env.unwrapped.s\n",
        "    if not option.can_start(state):\n",
        "        raise KeyError\n",
        "\n",
        "    # Действуем в соответствии со стратегией навыка до тех пор,\n",
        "    # пока либо навык, либо эпизод не завершится.\n",
        "    # В процессе копим дисконтированную сумму вознаграждений.\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    # отрисовка для отображения результата выполнения навыка\n",
        "    if debug:\n",
        "        print(env.render())\n",
        "\n",
        "    return s, reward, terminated, truncated, obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8Fb6DxdqUdO"
      },
      "source": [
        "Итак, теперь у нас есть 4 универсальных навыка. Каждый навык позволяет агенту переместиться из любой точки в целевую (R, G, Y или B). Тогда, чтобы решить любую поставленную задачу, алгоритм работы агента мог бы быть таким:\n",
        "- определить, в какой из целевых точек находится пассажир\n",
        "- выбрать и применить соответствующий навык, чтобы добраться до него\n",
        "- подобрать пассажира\n",
        "- определить, в какой из целевых точек находится его дом\n",
        "- выбрать и применить соответствующий навык, чтобы добраться до дома\n",
        "\n",
        "Давайте проверим работу навыков, вручную реализовав алгоритм выше:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwfsogpDqUdP"
      },
      "outputs": [],
      "source": [
        "checkpoints = ['R', 'G', 'Y', 'B']\n",
        "env = environment\n",
        "s, _ = env.reset()\n",
        "# определим номер точек, в которых находится пассажир и его дом\n",
        "_, _, person, goal = env.unwrapped.decode(s)\n",
        "print(\n",
        "    f'Task: go to {checkpoints[person]} for the person'\n",
        "    f' and then ride him to {checkpoints[goal]}'\n",
        ")\n",
        "\n",
        "# применим навык, чтобы добраться до пассажира\n",
        "r = apply_option(options[person], 0.99, env, debug=True)\n",
        "# подберем его (действие 4)\n",
        "env.step(4)\n",
        "# применим следующий навык, чтобы довезти пассажира до дома\n",
        "apply_option(options[goal], 0.99, env, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijFFN4GLqUdP"
      },
      "source": [
        "Как видите, наш подход прекрасно справляется с первым этапом, но не работает на втором. Дело в том, что мы забыли рассмотреть вариант, когда пассажир находится в такси! Таким образом мы проигнорировали целое множество состояний — все позиции такси с пассажиром на борту. Переведем среду в состояние, где пассажира мы уже подобрали и посмотрим, как ведет себя одна из опций."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7DB2Ib9qUdP"
      },
      "outputs": [],
      "source": [
        "env = environment\n",
        "s, _ = env.reset()\n",
        "\n",
        "s = env.unwrapped.s = env.unwrapped.encode(\n",
        "    taxi_row=rng.integers(5),\n",
        "    taxi_col=rng.integers(5),\n",
        "    # loc 4 means \"in taxi\"\n",
        "    pass_loc=4,\n",
        "    dest_idx=rng.integers(4)\n",
        ")\n",
        "\n",
        "_, _, person, goal = env.unwrapped.decode(s)\n",
        "print(f'Task: ride to {checkpoints[goal]}')\n",
        "\n",
        "print(env.render())\n",
        "r = apply_option(options[goal], 0.99, env, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_nV9FRjqUdP"
      },
      "source": [
        "Видим, что опции не обучились действовать в такой ситуации.\n",
        "Исправим нашу функцию обучения так, чтобы опции работали корректно для всех возможных состояний среды и сгенерируем их заново."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2 балла)"
      ],
      "metadata": {
        "id": "LF7DBJMGswhh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkZEbdLtqUdP"
      },
      "outputs": [],
      "source": [
        "def train_episode_fix(env, agent, rng=None):\n",
        "    if rng is None or isinstance(rng, int):\n",
        "        rng = np.random.default_rng(rng)\n",
        "\n",
        "    ret = 0.0\n",
        "    s, _ = env.reset()\n",
        "\n",
        "    # Так как среда всегда инициализируется с такси\n",
        "    # без пассажира, отредактируем начальное состояние\n",
        "    # среды так, чтобы с вероятностью 50% пассажир уже\n",
        "    # был в такси.\n",
        "    # NB: используйте env.decode(s), env.uwrapped.s,\n",
        "    # env.encode(row, col, pas, dest).\n",
        "    # NB2: pas == 4 означает \"в такси\"\n",
        "    ####### Здесь ваш код ########\n",
        "    raise NotImplementedError\n",
        "    ##############################\n",
        "\n",
        "    while True:\n",
        "        a = agent.act(s)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "        agent.update(s, a, next_s, r, terminated)\n",
        "        s = next_s\n",
        "        ret += r\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    return ret\n",
        "\n",
        "\n",
        "# Перезапустим обучение стратегий агентов\n",
        "for index in range(4):\n",
        "    for _ in range(7000):\n",
        "        wrapped_env = FixedTargetTaxi(\n",
        "            env=environment,\n",
        "            target_id=index, target_reward=50\n",
        "        )\n",
        "        train_episode_fix(\n",
        "            env=wrapped_env,\n",
        "            agent=agents_for_options[index],\n",
        "            rng=rng\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpZQjfmwqUdP"
      },
      "source": [
        "Запустим данную ячейку несколько раз и убедимся, что агент обучился для всех случаев!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_tzvIKHqUdQ"
      },
      "outputs": [],
      "source": [
        "env = environment\n",
        "s, _ = env.reset()\n",
        "\n",
        "s = env.unwrapped.s = env.unwrapped.encode(\n",
        "    taxi_row=rng.integers(5),\n",
        "    taxi_col=rng.integers(5),\n",
        "    pass_loc=rng.integers(5),\n",
        "    dest_idx=rng.integers(4)\n",
        ")\n",
        "\n",
        "_, _, person, goal = env.unwrapped.decode(s)\n",
        "print(f'Task: ride to {checkpoints[goal]}')\n",
        "\n",
        "print(env.render())\n",
        "r = apply_option(options[goal], 0.99, env, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z5_gLKnqUdQ"
      },
      "source": [
        "И еще раз проверим ручное решение задачи:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "YNn8AyTGqUdQ"
      },
      "outputs": [],
      "source": [
        "checkpoints = ['R', 'G', 'Y', 'B']\n",
        "env = environment\n",
        "s, _ = env.reset()\n",
        "print(env.render())\n",
        "\n",
        "# определим номер точек, в которых находится пассажир и его дом\n",
        "_, _, person, goal = env.unwrapped.decode(s)\n",
        "print(\n",
        "    f'Task: go to {checkpoints[person]} for the person'\n",
        "    f' and then ride him to {checkpoints[goal]}'\n",
        ")\n",
        "\n",
        "# применим навык, чтобы добраться до пассажира\n",
        "r = apply_option(options[person], 0.99, env, debug=True)\n",
        "# подберем его (действие 4)\n",
        "env.step(4)\n",
        "# применим следующий навык, чтобы довезти пассажира до дома\n",
        "apply_option(options[goal], 0.99, env, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSElxa0LqUdQ"
      },
      "source": [
        "Теперь все работает идеально!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6moDYcyqUdQ"
      },
      "source": [
        "### Объединение умений в иерархию\n",
        "\n",
        "Для реализации иерархической архитектуры со стратегией верхнего уровня над навыками, для начала, добавим поддержку элементарных навыков для посадки и высадки пассажира. Эти навыки могут быть начаты в любом состоянии, выполняют всегда только фиксированное действие и длятся один шаг, то есть завершаются с вероятностью 100% в любом состоянии."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvOLeo-GqUdQ"
      },
      "outputs": [],
      "source": [
        "class FixedActionAgent:\n",
        "    \"\"\"Стратегия с фиксированным действием.\"\"\"\n",
        "    def __init__(self, action):\n",
        "        self.action = action\n",
        "\n",
        "    def act(self, state, greedy=True):\n",
        "        return self.action\n",
        "\n",
        "    def update(*args, **kwargs):\n",
        "        pass\n",
        "\n",
        "options = options[:4]\n",
        "state_space = list(range(environment.observation_space.n))\n",
        "for action in [4, 5]:\n",
        "    # элементарный навык начинается в любом состоянии,\n",
        "    # выполняет фиксированное действие и сразу завершается\n",
        "    initial_states = set(state_space)\n",
        "    termination_prob = {s: 1.0 for s in state_space}\n",
        "    option = Option(\n",
        "        policy=FixedActionAgent(action),\n",
        "        initial_states=initial_states, termination_prob=termination_prob,\n",
        "    )\n",
        "    options.append(option)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u13qpANqUdR"
      },
      "source": [
        "Проверим элементарные навыки, переписав наш прежний ручной алгоритм решения задачи теперь уже только с использованием навыков."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWbOM4n6qUdR"
      },
      "outputs": [],
      "source": [
        "checkpoints = ['R', 'G', 'Y', 'B']\n",
        "env = environment\n",
        "s, _ = env.reset()\n",
        "print(env.render())\n",
        "\n",
        "# определим номер точек, в которых находится пассажир и его дом\n",
        "_, _, person, goal = env.unwrapped.decode(s)\n",
        "print(\n",
        "    f'Task: go to {checkpoints[person]} for the person'\n",
        "    f' and then ride him to {checkpoints[goal]}'\n",
        ")\n",
        "\n",
        "# применим навык, чтобы добраться до пассажира\n",
        "r = apply_option(options[person], 0.99, env, debug=True)\n",
        "# подберем его (элементарный навык 4)\n",
        "apply_option(options[4], 0.99, env, debug=True)\n",
        "# применим следующий навык, чтобы довезти пассажира до дома\n",
        "apply_option(options[goal], 0.99, env, debug=True)\n",
        "# высадим его (элементарный навык 5)\n",
        "apply_option(options[5], 0.99, env, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjwKVgC1qUdR"
      },
      "source": [
        "Наконец, реализуем обертку для окружения, которая вместо элементарных действий исполняет навыки. Для этого в качестве параметров инициализации обертки передадим ей список обученных ранее навыков. Таким образом мы как бы скроем от стратегии верхнего уровня наличие нижнего уровня --- для нее навыки будут выглядеть как обычные элементарные действия, выполняемые за один шаг взаимодействия со средой, а единственный наблюдаемый эффект длительности навыков будет в выдаваемом вознаграждении."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3A-viy2qUdR"
      },
      "outputs": [],
      "source": [
        "class OptionTaxiStepWrapper(gym.Wrapper):\n",
        "    \"\"\"В качестве элементарных действий выполняет навыки.\"\"\"\n",
        "    def __init__(self, env, options, gamma=1.0):\n",
        "        self.options = options\n",
        "        self.gamma = gamma\n",
        "        super().__init__(env)\n",
        "\n",
        "    def _step(self, action):\n",
        "        state, reward, terminated, truncated, obs = apply_option(\n",
        "            self.options[action], self.gamma, self.unwrapped\n",
        "        )\n",
        "        return state, reward, terminated, truncated, obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpoZZgN3qUdR"
      },
      "source": [
        "Обучим стратегию верхнего уровня над навыками и выведем график качества обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaWB1a5gqUdR"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from collections import deque\n",
        "\n",
        "option_agent = QLearningAgent(\n",
        "    learning_rate=0.1, epsilon=0.05, gamma=0.95,\n",
        "    get_legal_actions=lambda s: range(len(options)),\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# создаем окружение, использующее опции\n",
        "env = OptionTaxiStepWrapper(\n",
        "    gym.make('Taxi-v3', max_episode_steps=100),\n",
        "    options=options\n",
        ")\n",
        "rewards, rew_window = [], deque(maxlen=50)\n",
        "for episode in range(4001):\n",
        "    train_episode(env=env, agent=option_agent)\n",
        "    rew_window.append(\n",
        "        train_episode(env=env, agent=option_agent, is_eval=True)\n",
        "    )\n",
        "    rewards.append(np.mean(rew_window))\n",
        "\n",
        "    if episode % 200 == 0:\n",
        "        clear_output(True)\n",
        "        plt.plot(rewards)\n",
        "        plt.show()\n",
        "\n",
        "hrl_results = rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-kH8_oLqUdR"
      },
      "outputs": [],
      "source": [
        "raw_agent = QLearningAgent(\n",
        "    learning_rate=0.1, epsilon=0.05, gamma=0.95,\n",
        "    get_legal_actions=lambda s: range(6),\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# создаем окружение, использующее опции\n",
        "env = gym.make('Taxi-v3', max_episode_steps=100)\n",
        "rewards, rew_window = [], deque(maxlen=50)\n",
        "for episode in range(4001):\n",
        "    train_episode(env=env, agent=raw_agent)\n",
        "    rew_window.append(\n",
        "        train_episode(env=env, agent=raw_agent, is_eval=True)\n",
        "    )\n",
        "    rewards.append(np.mean(rew_window))\n",
        "\n",
        "    if episode % 200 == 0:\n",
        "        clear_output(True)\n",
        "        plt.plot(rewards)\n",
        "        plt.show()\n",
        "\n",
        "rl_results = rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "LSIBsIHhqUdW"
      },
      "outputs": [],
      "source": [
        "plt.plot(hrl_results, 'blue')\n",
        "plt.plot(rl_results, 'gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF8Vs9WRqUdW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}